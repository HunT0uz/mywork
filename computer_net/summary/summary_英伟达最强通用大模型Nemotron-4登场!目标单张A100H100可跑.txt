标题: 英伟达最强通用大模型Nemotron-4登场!目标单张A100/H100可跑
链接: https://baijiahao.baidu.com/s?id=1792220761040477405&wfr=spider&for=pc
总结: 编辑：桃子最近，英伟达团队推出了全新的模型Nemotron-4，150亿参数，在8T token上完成了训练。值得一提的是，Nemotron-4在英语、多语言和编码任务方面令人印象深刻。论文地址：https://arxiv.org/abs/2402.16819在7个评估基准上，与同等参数规模的模型相比，Nemotron-4 15B表现出色。甚至，其性能超过了4倍大的模型，以及专用于多语言任务的模型。如今LLM已经非常多了，英伟达新发布的语言模型，有何不同？打造最强通用LLM，单个A100/H100可跑最近发表的LLM研究受到了Chinchilla模型「缩放定律」的启发——给定固定计算预算，数据和模型大小一同优化。而过去，研究主要针对模型大小进行缩放。研究表明，给定两个数据分布类似的IsoFLOP GPT模型，一个是在1.4万亿token上的65亿参数模型，另一个是3000亿token上的2800亿参数模型。显然，65B的模型在下游任务上的准确性更高。从推理的角度来看，将计算分配给更多数据的训练，而不是增加模型大小特别有吸引力，可以减少延迟和服务模型所需的计算量。因此，语言建模训练工作的主要焦点已转向从CommonCrawl等公共资源中，收集高质量的数万亿token数据集。对此，英伟达研究人员提出了Nemotron-4 15B，来延续这一趋势。具体来说，Nemotron-4 15B是在8万亿个token，包括英语、多语种、编码文本的基础上进行训练。英伟达称，Nemotron-4 15B的开发目的：成为能在单个英伟达A100或H100 GPU上运行的最佳「通用大模型」。架构介绍Nemotron-4采用了标准的纯解码器Transformer架构，并带有因果注意掩码。核心的超参数，如表1所示。Nemotron-4有32亿个嵌入参数和125亿个非嵌入参数。研究人员使用旋转位置编码（RoPE）、SentencePiece分词器、MLP层的平方ReLU激活、无偏置项（bias terms）、零丢失率，以及无限制的输入输出嵌入。通过分组查询关注（GQA），可实现更快的推理和更低的内存占用。数据研究人员在包含8万亿个token的预训练数据集上训练Nemotron-4 15B。分为三种不同类型的数据：英语自然语言数据（70%）、多语言自然语言数据（15%）和源代码数据（15%）。英语语料库由来自各种来源和领域的精选文档组成，包括网络文档、新闻文章、科学论文、书籍等。代码和多语言数据包括一组多样化的自然语言和编程语言。研究人员发现，从这些语言中适当地采样token是在这些领域获得高准确度的关键。此外，研究人员分别在图3和图4中共享预训练数据集中用于代码和多语言标记的分布。预训练Nemotron-4使用384个DGX H100节点进行训练。每个节点包含8个基于英伟达Hopper架构的H100 80GB SXM5 GPU。在执行无稀疏性的16位浮点（bfloat16）算术时，每个H100 GPU的峰值吞吐量为989 teraFLOP/s。每个节点内，GPU通过NVLink和NVSwitch（nvl）连接；GPU到GPU的带宽为900 GB/s（每个方向450 GB/s）。每个节点都有8个NVIDIA Mellanox 400 Gbps HDR InfiniBand主机通道适配器（HCA），用于节点间通信。研究人员使用8路张量并行和数据并行的组合来训练模型，还使用了分布式优化器，将优化器状态分片到数据并行副本上。随着批大小的增加，数据并行度从96增加到384。表2总结了批大小提升的3个阶段，包括每次迭代时间和模型FLOP/s利用率（MFU）。MFU量化了GPU在模型训练中的利用效率。训练大约在13天内完成。再训练与最近的研究类似，研究人员发现在模型训练结束时，切换数据分布和学习率衰减时间表，可以极大地提高模型质量。具体来说，在对整个8T预训练数据集进行训练之后，使用相同的损失目标，并对与预训练token相比的较少的token进行持续训练。在这一额外的继续训练阶段，利用两种不同的数据分布。第一个分布是，从持续训练期间大部分token采样。它利用在预训练期间已经引入的token，但其分布将更大的采样权重放在更高质量来源上。第二个分布，引入了少量基准式对齐示例，以更好地让模型在下游评估中回答此类问题，同时还增加来自模型性能较低区域的数据源的权重。实验结果研究人员在涵盖各种任务和领域的下游评估领域评了 Nemotron-4 15B。常识推理作者使用LM-Evaluation Harness在所有上述任务中评估Nemotron-4 15B。表3显示了Nemotron-4 15B在这组不同的任务中实现了最强的平均性能。热门的综合基准从表4可以看出，Nemotron-4 15B在现有模型中获得了BBH的最佳分数，增长了近7%。此外，Nemotron-4在BBH基准测试中明显优于LLaMA-2 70B模型，其中LLaMA-2 70B的得分为51.2，Nemotron-4的得分为58.7。Nemotron-4 15B另外还获得了极具竞争力的MMLU分数。数学和代码表5重点介绍了Nemotron-4 15B在数学和代码任务上的性能。具体来说，在数学推理上，Nemotron-4 15B表现强劲，得分与Gemma 7B相似，但落后于Baichuan-2和QWEN等模型。在代码任务中，Nemotron-4的性能与QWEN 14B相当，但略落后于Gemma 7B。在这两种类型的任务中，Nemotron-4 15B的性能均优于Mistral 7B和LlaMA-213B/34B。几乎所有类似规模的开放模型都只根据Python相关任务的性能来确定其代码能力，而忽略了对其他编程语言能力的评估。在表6中，展示了Nemotron-4 15B在Multiple-E基准上的结果，涉及11种不同的编程语言。结果发现，Nemotron-4 15B在各种编程语言中都有很强的编码性能，平均性能优于Starcoder和Mistral 7B。研究人员特别强调了Nemotron-4 15B在Scala、Julia和R等低资源编程语言上的卓越性能。多语言分类在表7中，可以清楚地看到Nemotron-4在所有模型中实现了最佳性能，在4次设置中实现了近12%的改进。生成表8显示Nemotron-4 15B实现了最佳性能。令人印象深刻的是，Nemotron-4 15B能够显著改进下一个最佳模型PaLM 62B-cont。表9显示了MGSM上的性能，进一步证明了Nemotron-4 15B令人印象深刻的多语言能力。在这项评估数学和多语言能力交集的挑战性任务中，Nemotron-4 15B在比较模型中实现了最佳性能，并且比最接近的分数提高了近30%。机器翻译如表10所示，Nemotron-4 15B的性能远远优于LLaMA-2 13B和Baichuan-2 13B，性能分别提高了90.2%和44.1%。Nemotron-4 15B不仅在中文翻译成英文方面表现出色，而且在中文直接翻译成其他语言方面也能取得令人印象深刻的效果。这种能力凸显了Nemotron-4 15B对广泛的自然语言的深刻理解。编辑：桃子编辑：桃子最近，英伟达团队推出了全新的模型Nemotron-4，150亿参数，在8T token上完成了训练。最近，英伟达团队推出了全新的模型Nemotron-4，150亿参数，在8T token上完成了训练。值得一提的是，Nemotron-4在英语、多语言和编码任务方面令人印象深刻。值得一提的是，Nemotron-4在英语、多语言和编码任务方面令人印象深刻。论文地址：https://arxiv.org/abs/2402.16819论文地址：https://arxiv.org/abs/2402.16819在7个评估基准上，与同等参数规模的模型相比，Nemotron-4 15B表现出色。在7个评估基准上，与同等参数规模的模型相比，Nemotron-4 15B表现出色。甚至，其性能超过了4倍大的模型，以及专用于多语言任务的模型。甚至，其性能超过了4倍大的模型，以及专用于多语言任务的模型。如今LLM已经非常多了，英伟达新发布的语言模型，有何不同？如今LLM已经非常多了，英伟达新发布的语言模型，有何不同？打造最强通用LLM，单个A100/H100可跑打造最强通用LLM，单个A100/H100可跑最近发表的LLM研究受到了Chinchilla模型「缩放定律」的启发——给定固定计算预算，数据和模型大小一同优化。最近发表的LLM研究受到了Chinchilla模型「缩放定律」的启发——给定固定计算预算，数据和模型大小一同优化。而过去，研究主要针对模型大小进行缩放。而过去，研究主要针对模型大小进行缩放。研究表明，给定两个数据分布类似的IsoFLOP GPT模型，一个是在1.4万亿token上的65亿参数模型，另一个是3000亿token上的2800亿参数模型。研究表明，给定两个数据分布类似的IsoFLOP GPT模型，一个是在1.4万亿token上的65亿参数模型，另一个是3000亿token上的2800亿参数模型。显然，65B的模型在下游任务上的准确性更高。显然，65B的模型在下游任务上的准确性更高。从推理的角度来看，将计算分配给更多数据的训练，而不是增加模型大小特别有吸引力，可以减少延迟和服务模型所需的计算量。从推理的角度来看，将计算分配给更多数据的训练，而不是增加模型大小特别有吸引力，可以减少延迟和服务模型所需的计算量。因此，语言建模训练工作的主要焦点已转向从CommonCrawl等公共资源中，收集高质量的数万亿token数据集。因此，语言建模训练工作的主要焦点已转向从CommonCrawl等公共资源中，收集高质量的数万亿token数据集。对此，英伟达研究人员提出了Nemotron-4 15B，来延续这一趋势。对此，英伟达研究人员提出了Nemotron-4 15B，来延续这一趋势。具体来说，Nemotron-4 15B是在8万亿个token，包括英语、多语种、编码文本的基础上进行训练。具体来说，Nemotron-4 15B是在8万亿个token，包括英语、多语种、编码文本的基础上进行训练。英伟达称，Nemotron-4 15B的开发目的：英伟达称，Nemotron-4 15B的开发目的：成为能在单个英伟达A100或H100 GPU上运行的最佳「通用大模型」。架构介绍架构介绍Nemotron-4采用了标准的纯解码器Transformer架构，并带有因果注意掩码。Nemotron-4采用了标准的纯解码器Transformer架构，并带有因果注意掩码。核心的超参数，如表1所示。核心的超参数，如表1所示。Nemotron-4有32亿个嵌入参数和125亿个非嵌入参数。Nemotron-4有32亿个嵌入参数和125亿个非嵌入参数。研究人员使用旋转位置编码（RoPE）、SentencePiece分词器、MLP层的平方ReLU激活、无偏置项（bias terms）、零丢失率，以及无限制的输入输出嵌入。研究人员使用旋转位置编码（RoPE）、SentencePiece分词器、MLP层的平方ReLU激活、无偏置项（bias terms）、零丢失率，以及无限制的输入输出嵌入。通过分组查询关注（GQA），可实现更快的推理和更低的内存占用。通过分组查询关注（GQA），可实现更快的推理和更低的内存占用。数据数据研究人员在包含8万亿个token的预训练数据集上训练Nemotron-4 15B。研究人员在包含8万亿个token的预训练数据集上训练Nemotron-4 15B。分为三种不同类型的数据：英语自然语言数据（70%）、多语言自然语言数据（15%）和源代码数据（15%）。分为三种不同类型的数据：英语自然语言数据（70%）、多语言自然语言数据（15%）和源代码数据（15%）。英语语料库由来自各种来源和领域的精选文档组成，包括网络文档、新闻文章、科学论文、书籍等。英语语料库由来自各种来源和领域的精选文档组成，包括网络文档、新闻文章、科学论文、书籍等。代码和多语言数据包括一组多样化的自然语言和编程语言。代码和多语言数据包括一组多样化的自然语言和编程语言。研究人员发现，从这些语言中适当地采样token是在这些领域获得高准确度的关键。研究人员发现，从这些语言中适当地采样token是在这些领域获得高准确度的关键。此外，研究人员分别在图3和图4中共享预训练数据集中用于代码和多语言标记的分布。此外，研究人员分别在图3和图4中共享预训练数据集中用于代码和多语言标记的分布。预训练预训练Nemotron-4使用384个DGX H100节点进行训练。每个节点包含8个基于英伟达Hopper架构的H100 80GB SXM5 GPU。Nemotron-4使用384个DGX H100节点进行训练。每个节点包含8个基于英伟达Hopper架构的H100 80GB SXM5 GPU。在执行无稀疏性的16位浮点（bfloat16）算术时，每个H100 GPU的峰值吞吐量为989 teraFLOP/s。在执行无稀疏性的16位浮点（bfloat16）算术时，每个H100 GPU的峰值吞吐量为989 teraFLOP/s。每个节点内，GPU通过NVLink和NVSwitch（nvl）连接；GPU到GPU的带宽为900 GB/s（每个方向450 GB/s）。每个节点内，GPU通过NVLink和NVSwitch（nvl）连接；GPU到GPU的带宽为900 GB/s（每个方向450 GB/s）。每个节点都有8个NVIDIA Mellanox 400 Gbps HDR InfiniBand主机通道适配器（HCA），用于节点间通信。每个节点都有8个NVIDIA Mellanox 400 Gbps HDR InfiniBand主机通道适配器（HCA），用于节点间通信。研究人员使用8路张量并行和数据并行的组合来训练模型，还使用了分布式优化器，将优化器状态分片到数据并行副本上。随着批大小的增加，数据并行度从96增加到384。研究人员使用8路张量并行和数据并行的组合来训练模型，还使用了分布式优化器，将优化器状态分片到数据并行副本上。随着批大小的增加，数据并行度从96增加到384。表2总结了批大小提升的3个阶段，包括每次迭代时间和模型FLOP/s利用率（MFU）。MFU量化了GPU在模型训练中的利用效率。训练大约在13天内完成。表2总结了批大小提升的3个阶段，包括每次迭代时间和模型FLOP/s利用率（MFU）。MFU量化了GPU在模型训练中的利用效率。训练大约在13天内完成。再训练再训练与最近的研究类似，研究人员发现在模型训练结束时，切换数据分布和学习率衰减时间表，可以极大地提高模型质量。与最近的研究类似，研究人员发现在模型训练结束时，切换数据分布和学习率衰减时间表，可以极大地提高模型质量。具体来说，在对整个8T预训练数据集进行训练之后，使用相同的损失目标，并对与预训练token相比的较少的token进行持续训练。具体来说，在对整个8T预训练数据集进行训练之后，使用相同的损失目标，并对与预训练token相比的较少的token进行持续训练。在这一额外的继续训练阶段，利用两种不同的数据分布。在这一额外的继续训练阶段，利用两种不同的数据分布。第一个分布是，从持续训练期间大部分token采样。它利用在预训练期间已经引入的token，但其分布将更大的采样权重放在更高质量来源上。第一个分布是，从持续训练期间大部分token采样。它利用在预训练期间已经引入的token，但其分布将更大的采样权重放在更高质量来源上。第二个分布，引入了少量基准式对齐示例，以更好地让模型在下游评估中回答此类问题，同时还增加来自模型性能较低区域的数据源的权重。第二个分布，引入了少量基准式对齐示例，以更好地让模型在下游评估中回答此类问题，同时还增加来自模型性能较低区域的数据源的权重。实验结果实验结果研究人员在涵盖各种任务和领域的下游评估领域评了 Nemotron-4 15B。研究人员在涵盖各种任务和领域的下游评估领域评了 Nemotron-4 15B。常识推理常识推理作者使用LM-Evaluation Harness在所有上述任务中评估Nemotron-4 15B。作者使用LM-Evaluation Harness在所有上述任务中评估Nemotron-4 15B。表3显示了Nemotron-4 15B在这组不同的任务中实现了最强的平均性能。表3显示了Nemotron-4 15B在这组不同的任务中实现了最强的平均性能。热门的综合基准热门的综合基准从表4可以看出，Nemotron-4 15B在现有模型中获得了BBH的最佳分数，增长了近7%。从表4可以看出，Nemotron-4 15B在现有模型中获得了BBH的最佳分数，增长了近7%。此外，Nemotron-4在BBH基准测试中明显优于LLaMA-2 70B模型，其中LLaMA-2 70B的得分为51.2，Nemotron-4的得分为58.7。此外，Nemotron-4在BBH基准测试中明显优于LLaMA-2 70B模型，其中LLaMA-2 70B的得分为51.2，Nemotron-4的得分为58.7。Nemotron-4 15B另外还获得了极具竞争力的MMLU分数。Nemotron-4 15B另外还获得了极具竞争力的MMLU分数。数学和代码数学和代码表5重点介绍了Nemotron-4 15B在数学和代码任务上的性能。表5重点介绍了Nemotron-4 15B在数学和代码任务上的性能。具体来说，在数学推理上，Nemotron-4 15B表现强劲，得分与Gemma 7B相似，但落后于Baichuan-2和QWEN等模型。具体来说，在数学推理上，Nemotron-4 15B表现强劲，得分与Gemma 7B相似，但落后于Baichuan-2和QWEN等模型。在代码任务中，Nemotron-4的性能与QWEN 14B相当，但略落后于Gemma 7B。在代码任务中，Nemotron-4的性能与QWEN 14B相当，但略落后于Gemma 7B。在这两种类型的任务中，Nemotron-4 15B的性能均优于Mistral 7B和LlaMA-213B/34B。在这两种类型的任务中，Nemotron-4 15B的性能均优于Mistral 7B和LlaMA-213B/34B。几乎所有类似规模的开放模型都只根据Python相关任务的性能来确定其代码能力，而忽略了对其他编程语言能力的评估。几乎所有类似规模的开放模型都只根据Python相关任务的性能来确定其代码能力，而忽略了对其他编程语言能力的评估。在表6中，展示了Nemotron-4 15B在Multiple-E基准上的结果，涉及11种不同的编程语言。在表6中，展示了Nemotron-4 15B在Multiple-E基准上的结果，涉及11种不同的编程语言。结果发现，Nemotron-4 15B在各种编程语言中都有很强的编码性能，平均性能优于Starcoder和Mistral 7B。结果发现，Nemotron-4 15B在各种编程语言中都有很强的编码性能，平均性能优于Starcoder和Mistral 7B。研究人员特别强调了Nemotron-4 15B在Scala、Julia和R等低资源编程语言上的卓越性能。研究人员特别强调了Nemotron-4 15B在Scala、Julia和R等低资源编程语言上的卓越性能。多语言多语言分类分类在表7中，可以清楚地看到Nemotron-4在所有模型中实现了最佳性能，在4次设置中实现了近12%的改进。在表7中，可以清楚地看到Nemotron-4在所有模型中实现了最佳性能，在4次设置中实现了近12%的改进。生成生成表8显示Nemotron-4 15B实现了最佳性能。表8显示Nemotron-4 15B实现了最佳性能。令人印象深刻的是，Nemotron-4 15B能够显著改进下一个最佳模型PaLM 62B-cont。令人印象深刻的是，Nemotron-4 15B能够显著改进下一个最佳模型PaLM 62B-cont。表9显示了MGSM上的性能，进一步证明了Nemotron-4 15B令人印象深刻的多语言能力。表9显示了MGSM上的性能，进一步证明了Nemotron-4 15B令人印象深刻的多语言能力。在这项评估数学和多语言能力交集的挑战性任务中，Nemotron-4 15B在比较模型中实现了最佳性能，并且比最接近的分数提高了近30%。在这项评估数学和多语言能力交集的挑战性任务中，Nemotron-4 15B在比较模型中实现了最佳性能，并且比最接近的分数提高了近30%。机器翻译机器翻译如表10所示，Nemotron-4 15B的性能远远优于LLaMA-2 13B和Baichuan-2 13B，性能分别提高了90.2%和44.1%。如表10所示，Nemotron-4 15B的性能远远优于LLaMA-2 13B和Baichuan-2 13B，性能分别提高了90.2%和44.1%。Nemotron-4 15B不仅在中文翻译成英文方面表现出色，而且在中文直接翻译成其他语言方面也能取得令人印象深刻的效果。Nemotron-4 15B不仅在中文翻译成英文方面表现出色，而且在中文直接翻译成其他语言方面也能取得令人印象深刻的效果。这种能力凸显了Nemotron-4 15B对广泛的自然语言的深刻理解。这种能力凸显了Nemotron-4 15B对广泛的自然语言的深刻理解。举报/反馈
关键词: Nemotron-4,  15B,  8万亿token
AI技术: 旋转位置编码（RoPE）, 无偏置项（bias terms）, 无限制的输入输出嵌入。
行业: 英伟达,  英伟达,  英伟达
重大事件摘要: 这篇文章介绍了英伟达团队推出的新模型Nemotron-4 15B，这是一个具有150亿参数的大型通用大模型。该模型在英语、多语言和编码任务方面表现出色，尤其是在7个评估基准上与同等规模的模型相比，其性能更为优异。此外，Nemotron-4 15B的性能甚至超过了一些更大型的模型，如4倍大的模型以及专门用于多语言任务的模型。

文章还详细介绍了Nemotron-4 15B的技术架构和训练数据。该模型采用了标准的纯解码器Transformer架构，并在8万亿token（包括英语、多语言和编码文本）上进行训练。通过分组查询关注（GQA）实现更快的推理和更低的内存占用。研究人员使用了384个DGX H100节点来训练这个模型，每个节点包含8个基于英伟达Hopper架构的H100 80GB SXM5 GPU。

值得注意的是，英伟达声称Nemotron-4 15B能够在单个英伟达A100或H100 GPU上运行，成为能在单个A100/H100 GPU上运行的最佳通用大模型。这一特性使得该模型在实际应用中具有很高的灵活性和实用性。
