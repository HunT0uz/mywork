标题: 什么是深度学习?(从函数逼近论的角度来理解)
链接: http://www.baidu.com/link?url=2n8nVcLo30kHPbHzLUQ_qf_raK9NK5rf3sEUAbeTfgOUZrxsVAwkiMRkxvty2mMo9CMlB6byHLFKBS4L86OJCrat6CGWgXfLrECfchj1sWW
总结: 图32.各种人工3D形状描述子。图33.二维图像与三维网格的数据结构的区别。图34.三维几何数据的不同表达。&nbsp;十二、&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;智能图形处理：三维数据的深度学习&nbsp;3D图形作为一种新型的可视媒体数据类型，同样可以利用人工智能和深度学习的方法来进行分析和处理，融合大数据、并行计算及人工智能技术的最新进展以提高计算机图形学算法及系统易用性及效率，称为智能图形处理。近几年，已有越来越多的研究工作将机器学习和深度学习的方法应用于3D数据。本节仅就3D形状描述子方面做一个简略的介绍。应用深度学习来自动抽取3D形状特征主要有以下3种方法：第一种，基于传统的人工特征（维数一致了）来进行更抽象的形状特征的学习和抽取；这种方法是非端到端的；第二种，将3D数据转化为规整结构数据（欧氏区域），然后再应用深度学习方法抽取形状特征，称为显式方法；第三种，将深度神经网络改造成能够处理非欧氏区域数据，称为隐式方法。下面分别从方法论上进行简单介绍。&nbsp;12.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;12.1.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;基于人工特征的特征学习方法由于传统卷积无法直接作用于3D形状，早期的方法避开了直接以3D形状作为输入的训练，而是先从3D模型中提取一些人工特征（称为低级特征，如图32），再把这些特征输入到神经网络中进行学习，获得新的抽象特征（称为高级特征），如图35所示。图35.基于人工特征的特征学习方法。[12-1]ZhigeXie,KaiXu,LigangLiu,YueshanXiong.3DShapeSegmentationandLabelingviaExtremeLearningMachine.ComputerGraphicsForum(Proc.SGP),33(5):85-95,2014.[12-2]KanGuo,DongqingZou,andXiaowuChen.3Dmeshlabelingviadeepconvolutionalneuralnetworks.ACMTransactionsonGraphics,35,1,2015.[12-3]ZhenyuShu,ChengwuQi,ShiqingXin,ChaoHu,LiWang,YuZhang,LigangLiu.Unsupervised3DShapeSegmentationandCo-segmentationviaDeepLearning.ComputerAidedGeometricDesign(Proc.GMP),43:39-52,2016.人工定义的特征很大程度上取决于人的经验。因此，基于人工特征的特征学习方法并不是端到端的深度学习方法。另外，各种人工特征向量的排列也会改变特征矩阵及其卷积后的结果，对结果产生如何的影响也不清楚。&nbsp;12.2.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;非本征方法这种方法是将3D数据进行合适的变换，将其变换到一个欧氏空间的规则区域，以便适用于深度神经网络。此类方法改变了3D网格模型的形态，本质上也改变了数据分布空间，是为了适应传统CNN或MLP对拓扑的要求而提出的折衷解决方法，因此成为基于欧氏空间的学习方法或非本征方法，如图36所示。图36.非本征深度学习方法。变换的区域和方法主要有以下几种。(1)&nbsp;&nbsp;图像区域。最简单的方法就是将3D模型数据投影到平面形成图像。(a)&nbsp;&nbsp;&nbsp;多视图图像：将3D模型往多个视角方向投影生成多幅图像，比如[12-4,12-5]；(b)&nbsp;&nbsp;全景图像：将3D模型往平面投影生成一幅全景图，比如[12-6]；(c)&nbsp;&nbsp;&nbsp;参数化图像：将3D模型的2D参数化看成为一幅图像，比如[12-7,12-8]。(2)&nbsp;&nbsp;体素区域。(a)&nbsp;&nbsp;&nbsp;类比于图像中的像素，将3D形状看作三维体素网格的0，1分布，比如[12-9,12-10，12-11]。由于体素的个数是三次方增长，因此，体素的分辨率无法太大，一般连128x128x128的分辨率做起来都很困难。(b)&nbsp;&nbsp;只将模型表面用体素来表达，而省去内部体素的存储，并且利用八叉树的形式来存储，可以有效地提高体素的分辨率及模型的表达精度，比如[12-12]。(3)&nbsp;&nbsp;基元组合。三维形状还可以表示成一些基本单元的组合，如矩形块、圆柱、圆锥、球等，然后利用网络来学习这些基本体块的参数，比如[12-13,12-14]。(4)&nbsp;&nbsp;点云。直接将3D数据看成点的集合。将每个点看作一个神经元节点，节点包含点的坐标或法向等信息，然后利用深度神经网络提取点的特征。这里要克服点及点邻域的顺序无关性等。比如[12-14,12-15,12-16,12-18]。[12-4]Su,H.andS.Maji,etal.(2015).Multi-viewconvolutionalneuralnetworksfor3Dshaperecognition.ICCV.[12-5]Xie,Z.andK.Xu,etal.(2015).ProjectiveFeatureLearningfor3DShapeswithMulti‐ViewDepthImages.CGF.&nbsp;[12-6]Shi,B.andS.Bai,etal.(2015).Deeppano:Deeppanoramicrepresentationfor3‐Dshaperecognition.IEEESignalProcessingLetters.&nbsp;[12-7]Sinha,A.,Bai,J.,Ramani,K.,etal.(2016).Deeplearning3Dshapesurfacesusinggeometryimages.ECCV.[12-8]Maron,H.andGalun,M.(2017).ConvolutionalNeuralNetworksonSurfacesviaSeamlessToricCovers.SIGGRAPH.[12-9]Qi,C.R.andH.Su,etal.(2016).Volumetricandmulti-viewcnnsforobjectclassificationon3Ddata.CVPR.[12-10]Brock,A.andT.Lim,etal.(2016).Generativeanddiscriminativevoxelmodelingwithconvolutionalneuralnetworks.NIPS.[12-11]Wu,Z.,andSong,S.etal.(2015).3dshapenets:adeeprepresentationforvolumetricshapes.CVPR.[12-12]Wang,P.S.&nbsp;andLiu,Y.etal.(2017).O-cnn:octree-basedconvolutionalneuralnetworksfor3dshapeanalysis.TOG.[12-13]J.Li,K.Xu,etal.Grass:Generativerecrusiveautoencodersforshapestructures.ACMTrans.onGraph.,36(4),2017.[12-14]S.Tulsianietal.Learningshapeabstractionbyassemblingvolumetricprimitives.CVPR,2017.[12-15]Garcia-Garcia,A.andGomez-Donoso,F.etal.(2016).PointNet:A3DConvolutionalNeuralNetworkforreal-timeobjectclassrecognition.IJCNN.[12-16]Qi,C.R.andH.Su,etal.(2017).PointNet:Deeplearningonpointsetsfor3Dclassificationandsegmentation.CVPR.[12-17]Qi,C.R.andYi,L.etal.(2017).PointNet++:DeepHierarchicalFeatureLearningonPointSetsinaMetricSpace.CVPR.[12-18]YangyanLi,RuiBu,MingchaoSun,andBaoquanChen(2018).PointCNN.arXiv:1801.07791.&nbsp;12.3.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;本征方法本征方法直接将三维形状看成二维流形（Manifold）或由点组成的图（Graph），顶点之间的距离不再是欧氏距离，然后直接将卷积定义在这样的数据结构上，称为非欧氏空间学习的方法或本征方法，如图37。两种典型的网络为测地卷积网络（GeodesicCNN,GCNN）和图卷积网络（Graph-basedCNN），如图38，可参考[12-19,12-20,12-21,12-22]。[12-19]Masci,J.andBoscaini,D.etal.(2015).GeodesicconvolutionalneuralnetworksonRiemannianmanifolds.ICCV.[12-20]Monti,F.andBoscaini,D.,Masci.(2016).GeometricdeeplearningongraphsandmanifoldsusingmixturemodelCNNs.arXivpreprintarXiv:1611.08402.[12-21]Boscaini,D.andMasci,J.etal.(2016).Learningshapecorrespondencewithanisotropicconvolutionalneuralnetworks.NIPS.[12-22]Yi,L.andSu,H.etal.(2017).Syncspeccnn:SynchronizedspectralCNNfor3dshapesegmentation.CVPR.图37.本征深度学习方法。&nbsp;&nbsp;&nbsp;图38.本征深度学习的具体方法。左：测地卷积；右：图卷积网络。&nbsp;12.4.&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;端到端的生成模型在上面的工作中，深度学习用于解决3D形状的识别、分割、匹配和对应等问题。近年来，越来越多的工作致力于3D模型的构建和生成（大部分都是使用生成-对抗网络GAN）。由于可以自动生成大量的3D模型，对于扩充3D模型数据集具有重要的意义，非常值得大家关注。这里简略介绍一下端到端的3D模型生成的一些工作。所谓端到端的生成模型，就是输入是简单易获得的，输出是三维模型，中间无需其他操作。例如，可以从输入的一张照片输出三维体素模型[12-23,12-24]、点云模型[12-25]、或者网格模型及其参数化[12-26]；[12-27]利用级联的图卷积网络由粗到精逐渐细化网格模型；[12-28]由手绘草图生成参数化的夸张人脸模型。[12-23]J.Wuetal.Learningaprobabilisticlatentspaceofobjectshapesvia3Dgenerative-adversarialmodeling.AdvancesinNeuralInformationProcessingSystems,82-90,2016.[12-24]J.Lietal.Grass:Generativerecursiveautoencodersforshapestructures.Siggraph2017.[12-25]H.Fanetal.Apointsetgenerationnetworkfor3Dobjectreconstructionfromasingleimage.CVPR2017.[12-26]T.Groueixetal.AltasNet:Apapierapproachtolearning3Dsurfacegeneration.arXiv:1802.05384,2018.[12-27]N.Wangetal.Pixel2Mesh:Generating3DmeshmodelsformsingleRGBiamges.arXiv:1804.01654.[12-28]X.Hanetal.DeepSketch2Face:Adeeplearningbasedsketchingsystemsfor3Dfaceandcaricaturemodeling.Siggraph2017.&nbsp;【后记】此文分享了笔者对基于深度神经网络的深度学习的理解，从逼近论的角度深入浅出地理解了深度学习的原理，了解了整个调试深度网络的“炼丹”过程，是个需要丰富经验的“技术活”。机器学习还有很多其他重要的方法，比如非监督学习和强化学习等，笔者就不一一进行介绍了。从本文的分析可知，基于深度神经网络的深度学习背后的运行原理主要基于数据驱动的函数拟合，采用的是“经验主义”的实用方法（比如数学中的最小二乘方法及统计学中的统计回归方法等）；它离真正的人工智能，即能通过图灵测试（TuringTesting）的智能机器还很远。从数学本质上来看，现在基于数据的深度学习方法实质上只有“记忆”能力，并没有对问题的“理解”能力，更没有人们所期待的“人类智能”。从研究的方法论来看，对于要研究的对象（信息或数据），如果在不同的空间（角度）来观察会有对该对象有不同的理解和认知（比如信号处理中，在时域和频域的变换下的Fourier分析），即对象在该空间中所表现的“特征”。因此，在不同的空间进行变换就成为关键的步骤之一。在以往的研究中，需要研究者对对象有深刻的理解来对信息进行特定的变换（即在特定的基函数的变换下）。而深度神经网络实现了端到端的变换，通过多层（深度）复合的变换（多次变换）自动提取了在不同空间的“特征”，即根据数据本身自适应“学习”基函数。这是深度神经网络的巨大优势之一。另外，深度神经网络的另一个巨大优势就是可以通过数量众多的小函数（激活函数）的线性变换及复合来逼近非常复杂的函数，解决了人工设计基函数的困难。因此，基于深度神经网络的深度学习具有非常广泛的实际应用。另外值得一提的是，从Fourier变换到小波变换的发展来看，可以将神经网络中的激活函数变为特殊的局部基函数，使得比现有的全局激活函数适用于更广泛的函数（比如瞬时信号函数），将来再抽时间来详细分析和阐述。从计算数学领域来看，神经网络提供了可以构建任意维度之间的映射和函数，这是传统逼近论所无法做到的。这使得人们研究的数据维度和数据类型极大的丰富了，而不只是对1维和2维等低维数据进行分析和处理。这将是一个非常重要的方法论，将对科学领域的研究方法、手段和进展也具有极大的促进和推动作用。有理由相信，深度学习方法（或人工智能方法）将会被当作计算数学、应用科学的计算工具或科研工具，催生新的科研范式，被越来越多地用来解决一些计算数学和其他科学领域中的问题。从方法论来看，也可以从统计学的角度来看深度神经网络，将深度神经网络来拟合数据的分布（分布函数），比如生成网络和GAN等，也是近期研究的热点。在本文中对此方法论没有进行展开。图39.Gartner2018人工智能技术成熟度曲线。（图片来源于互联网）&nbsp;五年前（2013年4月），《麻省理工学院技术评论》杂志将深度学习列为2013年十大突破性技术（BreakthroughTechnology）之首。经过前几年的高速发展，根据Gartner在7月刚发布的2018人工智能技术成熟度曲线（如图39），深度神经网络及深度学习已从前几年的爬坡阶段到达顶部。未来，相信人们将会更冷静地来看待及发展相关的技术和应用。最近，有人提出“可微编程（DifferentiableProgramming）”的概念，就是将神经网络当成一种语言（而不是一个简单的机器学习的方法），来描述我们客观世界的规律。甚至YannLeCun曾在Facebook的文章中说道：“DeepLearningIsDead.LongLiveDifferentiableProgramming!”（深度学习已死，可微编程永生）这个概念的提出又将神经网络提高了一个层次。本文完稿核对时，笔者发现文中有些数学符号（比如等）存在着重复定义与使用，但根据上下文并不影响阅读和理解，因此笔者就不花时间去修正了，在此说明下。再次声明下，笔者的主要研究领域为计算机图形学，而非人工智能领域，因此本文仅仅为笔者从外行的角度对基于深度神经网络的深度学习的粗浅理解，而非人工智能领域对深度神经网络和深度学习的权威解释。笔者对其中的有些内容的理解是有限的，甚至是有误的。因此，该文仅供读者参考，不作为专业资料！如有不当之处，还请读者指正！&nbsp;【致谢】笔者在学习深度神经网络及深度学习的过程中，阅读了许多相关的书籍和论文（比如周志华老师的《机器学习》一书和李宏毅老师的课件等）以及微信订阅号（比如《SigAI》、《人工智能学家》、《深度学习大讲堂》、《老顾谈几何》等），在此表示感谢。也感谢舒振宇、徐凯、韩晓光、沈小勇、胡瑞珍等同行、以及笔者实验室的同事（张举勇、傅孝明、杨周旺等）和学生（陈明佳、方清、杜冬、石磊、欧阳文清、刘中远等），与他们的交流和讨论也让笔者受益匪浅。&nbsp;&nbsp;最后，希望您能从本文受益。祝您健康、快乐、成功！&nbsp;刘利刚中国科学技术大学图形与几何计算实验室(http://gcl.ustc.edu.cn)个人主页：http://staff.ustc.edu.cn/~lgliu电子邮箱：lgliu@ustc.edu.cn2018年8月8日&nbsp;
关键词: 
AI技术: 
行业: 
重大事件摘要: 
