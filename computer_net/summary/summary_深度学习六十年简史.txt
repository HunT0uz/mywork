标题: 深度学习六十年简史
链接: http://www.baidu.com/link?url=DOjYqUESmv52mPX3RcBv3ILFmMZDlB-pKTGCXT9tbvhgFrJjH7H8M35R-IIQ1GBzf7Yo3w4iNRhNJv3HvM3YfnikP1f9VvL2oPG2hRCpplT436TAghFJgZ6ZiBrrSxy8tFcN9Gin8yxQHK3GfGcD8PtaoK5nIpZ_efS18d0ZgHh3GO41mdw3sj87Jz9Q1zaSz2RvC9WJ1mSkKXgHVeMPaZIF6KY6Tf9HhRvHrNbuRrekhJHNpEbOsnbbAWYsWz3JlzswvICcmTDmBVazSqflSK
总结: 11958年：感知机的兴起1958年，弗兰克·罗森布拉特发明了感知机，这是一种非常简单的机器模型，后来成为当今智能机器的核心和起源。感知机是一个非常简单的二元分类器，可以确定给定的输入图像是否属于给定的类。为了实现这一点，它使用了单位阶跃激活函数。使用单位阶跃激活函数，如果输入大于0，则输出为1，否则为0。下图是感知机的算法。感知机Frank的意图不是将感知机构建为算法，而是构建成一种机器。感知机是在名为MarkI感知机的硬件中实现的。MarkI感知机是一台纯电动机器。它有400个光电管（或光电探测器），其权重被编码到电位器中，权重更新（发生在反向传播中）由电动机执行。下图是MarkI感知机。MarkI感知机。图片来自美国国家历史博物馆就像你今天在新闻中看到的关于神经网络的内容一样，感知机也是当时的头条新闻。《纽约时报》报道说，“[海军]期望电子计算机的初步模型能够行走、说话、观察、书写、自我复制并意识到它的存在”。今天，我们都知道机器仍然难以行走、说话、观察、书写、复制自己，而意识则是另一回事。MarkI感知机的目标仅仅是识别图像，而当时它只能识别两个类别。人们花了一些时间才知道添加更多层（感知机是单层神经网络）可以使网络具有学习复杂功能的能力。这进一步产生了多层感知机(MLP)。21982-1986:循环神经网络(RNN)在多层感知机显示出解决图像识别问题的潜力之后，人们开始思考如何对文本等序列数据进行建模。循环神经网络是一类旨在处理序列的神经网络。与多层感知机(MLP)等前馈网络不同，RNN有一个内部反馈回路，负责记住每个时间步长的信息状态。前馈网络与循环神经网络第一种RNN单元在1982年到1986年之间被发现，但它并没有引起人们的注意，因为简单的RNN单元在用于长序列时会受到很大影响，主要是存在记忆力短和梯度不稳定的问题。31998：LeNet-5，第一个CNN架构LeNet-5是最早的卷积网络架构之一，于1998年用于文档识别。LeNet-5由3个部分组成：2个卷积层、2个子采样或池化层和3个全连接层。卷积层中没有激活函数。正如论文所说，LeNet-5已进行商业化应用，每天读取数百万张支票。下面是LeNet-5的架构。该图像取自其原始论文。LeNet-5在当时确实是一个有影响力的研究，但它（常规的卷积网络）直到20年后才受到关注！LeNet-5建立在早期工作的基础上，例如福岛邦彦提出的第一个卷积神经网络、反向传播（Hinton等人，1986年）和应用于手写邮政编码识别的反向传播（LeCun等人，1989年）。41998：长短期记忆（LSTM）由于梯度不稳定的问题，简单RNN单元无法处理长序列问题。LSTM是可用于处理长序列的RNN版本。LSTM基本上是RNN单元的极端情况。LSTM单元的一个特殊设计差异是它有一个门机制，这是它可以控制多个时间步长的信息流的基础。简而言之，LSTM使用门来控制从当前时间步长到下一个时间步长的信息流，有以下4种方式：输入门识别输入序列。遗忘门去掉输入序列中包含的所有不相关信息，并将相关信息存储在长期记忆中。LTSM单元更新“更新单元“的状态值。输出门控制必须发送到下一个时间步长的信息。LSTM架构。图片取自MIT的课程《6.S191IntroductiontoDeepLearning》LSTM处理长序列的能力使其成为适合各种序列任务的神经网络架构，例如文本分类、情感分析、语音识别、图像标题生成和机器翻译。LSTM是一种强大的架构，但它的计算成本很高。2014年推出的GRU（GatedRecurrentUnit）可以解决这个问题。与LSTM相比，GRU的参数更少，效果也很好。52012年：ImageNet挑战赛、AlexNet和ConvNet的兴起如果跳过ImageNet大规模视觉识别挑战赛(ILSVRC)和AlexNet，就几乎不可能讨论神经网络和深度学习的历史。ImageNet挑战赛的唯一目标是评估大型数据集上的图像分类和对象分类架构。它带来了许多新的、强大的、有趣的视觉架构。挑战赛始于2010年，但在2012年发生了变化，AlexNet以15.3%的Top5低错误率赢得了比赛，这几乎是此前获胜者错误率的一半。AlexNet由5个卷积层、随后的最大池化层、3个全连接层和一个Softmax层组成。AlexNet提出了深度卷积神经网络可以很好地处理视觉识别任务的想法。但当时，这个观点还没有深入到其他应用上！在随后的几年里，ConvNets架构不断变得更大并且工作得更好。例如，有19层的VGG以7.3%的错误率赢得了挑战。GoogLeNet（Inception-v1）更进一步，将错误率降低到6.7%。2015年，ResNet（DeepResidualNetworks）扩展了这一点，并将错误率降低到3.6%，并表明通过残差连接，我们可以训练更深的网络（超过100层），在此之前，训练如此深的网络是不可能的。人们发现更深层次的网络做得更好，这导致产生了其他新架构，如ResNeXt、Inception-ResNet、DenseNet、Xception等。读者可以在这里找到这些架构和其他现代架构的总结和实现：https://github.com/Nyandwi/ModernConvNetsModernConvNets库ImageNet挑战赛。图片来自课程《CS231n》62014年:深度生成网络生成网络用于从训练数据中生成或合成新的数据样本，例如图像和音乐。生成网络有很多种类型，但最流行的是由IanGoodfellow在2014年创建的生成对抗网络(GAN)。GAN由两个主要组件组成：生成假样本的生成器，以及区分真实样本和生成器生成样本的判别器。生成器和鉴别器可以说是互相竞争的关系。他们都是独立训练的，在训练过程中，他们玩的是零和游戏。生成器不断生成欺骗判别器的假样本，而判别器则努力发现那些假样本（参考真实样本）。在每次训练迭代中，生成器在生成接近真实的假样本方面做得更好，判别器必须提高标准来区分不真实的样本和真实样本。GAN一直是深度学习社区中最热门的研究之一，该社区以生成伪造的图像和Deepfake视频而闻名。如果读者对GAN的最新进展感兴趣，可以阅读StyleGAN2、DualStyleGAN、ArcaneGAN和AnimeGANv2的简介。如需GAN资源的完整列表：https://github.com/nashory/gans-awesome-applications。下图说明了GAN的模型架构。生成对抗网络（GAN）GAN是生成模型的一种。其他流行的生成模型类型还有VariationAutoencoder(变分自编码器，VAE)、AutoEncoder（自编码器）和扩散模型等。72017年：Transformers和注意力机制时间来到2017年。ImageNet挑战赛结束了。新的卷积网络架构也被制作出来。计算机视觉社区的每个人都对当前的进展感到高兴。核心计算机视觉任务（图像分类、目标检测、图像分割）不再像以前那样复杂。人们可以使用GAN生成逼真的图像。NLP似乎落后了。但是随后出现了一些事情，并且在整个网络上都成为了头条新闻：一种完全基于注意力机制的新神经网络架构横空出世。并且NLP再次受到启发，在随后的几年，注意力机制继续主导其他方向（最显著的是视觉）。该架构被称为Transformer。在此之后的5年，也就是现在，我们在这里谈论一下这个最大的创新成果。Transformer是一类纯粹基于注意力机制的神经网络算法。Transformer不使用循环网络或卷积。它由多头注意力、残差连接、层归一化、全连接层和位置编码组成，用于保留数据中的序列顺序。下图说明了Transformer架构。图片来自于《AttentionIsAllYouNeed》Transformer彻底改变了NLP，目前它也在改变着计算机视觉领域。在NLP领域，它被用于机器翻译、文本摘要、语音识别、文本补全、文档搜索等。读者可以在其论文《AttentionisAllYouNeed》中了解有关Transformer的更多信息。82018年至今自2017年以来，深度学习算法、应用和技术突飞猛进。为了清楚起见，后来的介绍是按类别划分。在每个类别中，我们都会重新审视主要趋势和一些最重要的突破。VisionTransformersTransformer在NLP中表现出优异的性能后不久，一些勇于创新的人就迫不及待地将注意力机制用到了图像领域。在论文《AnImageisWorth16x16Words:TransformersforImageRecognitionatScale》中，谷歌的几位研究人员表明，对直接在图像块序列上运行的正常Transformer进行轻微修改，就可以在图像分类数据集上产生实质性的结果。他们将这种架构称为VisionTransformer(ViT)，它在大多数计算机视觉基准测试中都有不错表现（在作者撰写本文时，ViT是Cifar-10上最先进的分类模型）。ViT设计师并不是第一个尝试在识别任务中使用注意力机制的人。我们可以在论文AttentionAugmentedConvolutionalNetworks中找到第一个使用注意力机制的记录，这篇论文试图结合自注意力机制和卷积（摆脱卷积主要是由于CNN引入的空间归纳偏置）。另一个例子见于论文《VisualTransformers:Token-basedImageRepresentationandProcessingforComputerVision，这篇论文在基于滤波器的token或视觉token上运行Transformer。这两篇论文和许多其他未在此处列出的论文突破了一些基线架构（主要是ResNet）的界限，但当时并没有超越当前的基准。ViT确实是最伟大的论文之一。这篇论文最重要的见解之一是ViT设计师实际上使用图像patch作为输入表示。他们对Transformer架构没有太大的改变。VisionTransformer(ViT)除了使用图像patch之外，使VisionTransformer成为强大架构的结构是Transformer的超强并行性及其缩放行为。但就像生活中的一切一样，没有什么是完美的。一开始，ViT在视觉下游任务（目标检测和分割）上表现不佳。在引入SwinTransformers之后，VisionTransformer开始被用作目标检测和图像分割等视觉下游任务的骨干网络。SwinTransformer超强性能的核心亮点是由于在连续的自注意力层之间使用了移位窗口。下图描述了SwinTransformer和VisionTransformer（ViT）在构建分层特征图方面的区别。图片来自SwinTransformer原文VisionTransformer一直是近来最令人兴奋的研究领域之一。读者可以在论文《TransformersinVision:ASurvey》中了解更多信息。其他最新视觉Transformer还有CrossViT、ConViT和SepViT等。视觉和语言模型视觉和语言模型通常被称为多模态。它们是涉及视觉和语言的模型，例如文本到图像生成（给定文本，生成与文本描述匹配的图像）、图像字幕（给定图像，生成其描述）和视觉问答（给定一个图像和关于图像中内容的问题，生成答案）。很大程度上，Transformer在视觉和语言领域的成功促成了多模型作为一个单一的统一网络。实际上，所有视觉和语言任务都利用了预训练技术。在计算机视觉中，预训练需要对在大型数据集（通常是ImageNet）上训练的网络进行微调，而在NLP中，往往是对预训练的BERT进行微调。要了解有关V-L任务中预训练的更多信息，请阅读论文《ASurveyofVision-LanguagePre-TrainedModels》。有关视觉和语言任务、数据集的一般概述，请查看论文《TrendsinIntegrationofVisionandLanguageResearch:ASurveyofTasks,Datasets,andMethods》。前段时间，OpenAI发布了DALL·E2（改进后的DALL·E），这是一种可以根据文本生成逼真图像的视觉语言模型。现有的文本转图像模型有很多，但DALL·E2的分辨率、图像标题匹配度和真实感都相当出色。DALL·E2尚未对公众开放，以下是DALL·E2创建的一些图像示例。上面呈现的DALL·E2生成的图像取自一些OpenAI员工，例如@sama、@ilyasut、@model_mechanic和openaidalle。大规模语言模型(LLM)语言模型有多种用途。它们可用于预测句子中的下一个单词或字符、总结一段文档、将给定文本从一种语言翻译成另一种语言、识别语音或将一段文本转换为语音。开玩笑地说，发明Transformers的人必须为语言模型在朝着大规模参数化方向前进而受到指责（但实际上没有人应该受到责备，Transformers是过去十年中最伟大的发明之一，大模型令人震惊的地方在于：如果给定足够的数据和计算，它总能更好地工作）。在过去的5年中，语言模型的大小一直在不断增长。在引入论文《Attentionisallyouneed》一年后，大规模语言模型开始出现。2018年，OpenAI发布了GPT（GenerativePre-trainedTransformer），这是当时最大的语言模型之一。一年后，OpenAI发布了GPT-2，一个拥有15亿个参数的模型。又一年后，他们发布了GPT-3，它有1750亿个参数，用了570GB的文本来训练。这个模型有175B的参数，模型有700GB。根据lambdalabs的说法，如果使用在市场上价格最低的GPU云训练GPT-3，需要366年，花费460万美元！GPT-n系列型号仅仅是个开始。还有其他更大的模型接近甚至比GPT-3更大。如：NVIDIAMegatron-LM有8.3B参数；最新的DeepMindGopher有280B参数。2022年4月12日，DeepMind发布了另一个名为Chinchilla的70B语言模型，尽管比Gopher、GPT-3和Megatron-TuringNLG（530B参数）小，但它的性能优于许多语言模型。Chinchilla的论文表明，现有的语言模型是训练不足的，具体来说，它表明通过将模型的大小加倍，数据也应该加倍。但是，几乎在同一周内又出现了具有5400亿个参数的GooglePathways语言模型（PaLM）！Chinchilla语言模型代码生成模型代码生成是一项涉及补全给定代码或根据自然语言或文本生成代码的任务，或者简单地说，它是可以编写计算机程序的人工智能系统。可以猜到，现代代码生成器是基于Transformer的。可以确定地说，人们已经开始考虑让计算机编写自己的程序了（就像我们梦想教计算机做的所有其他事情一样），不过代码生成器是在OpenAI发布Codex后受到关注。Codex是在GitHub公共仓库和其他公共源代码上微调的GPT-3。OpenAI表示：“OpenAICodex是一种通用编程模型，这意味着它基本上可以应用于任何编程任务（尽管结果可能会有所不同）。我们已经成功地将它用于编译、解释代码和重构代码。但我们知道，我们只触及了可以做的事情的皮毛。”目前，由Codex支持的GitHubCopilot扮演着结对程序员的角色。在我使用Copilot后，我对它的功能感到非常惊讶。作为不编写Java程序的人，我用它来准备我的移动应用程序（使用Java）考试。人工智能帮助我准备学术考试真是太酷了！在OpenAI发布Codex几个月后，DeepMind发布了AlphaCode，这是一种基于Transformer的语言模型，可以解决编程竞赛问题。AlphaCode发布的博文称：“AlphaCode通过解决需要结合批判性思维、逻辑、算法、编码和自然语言理解的新问题，在编程竞赛的参与者中估计排名前54%。”解决编程问题（或一般的竞争性编程）非常困难（每个做过技术面试的人都同意这一点），正如Dzmitry所说，击败“人类水平仍然遥遥无期”。前不久，来自MetaAI的科学家发布了InCoder，这是一种可以生成和编辑程序的生成模型。更多关于代码生成的论文和模型可以在这里找到：https://paperswithcode.com/task/code-generation/codeless再次回到感知机在卷积神经网络和Transformer兴起之前的很长一段时间里，深度学习都围绕着感知机展开。ConvNets在取代MLP的各种识别任务中表现出优异的性能。视觉Transformer目前也展示出似乎是一个很有前途的架构。但是感知机完全死了吗？答案可能不是。在2021年7月，研究人员发表了两篇基于感知机的论文。一个是MLP-Mixer:Anall-MLPArchitectureforVision，另一个是PayAttentiontoMLPs（gMLP）.MLP-Mixer声称卷积和注意力都不是必需的。这篇论文仅使用多层感知机(MLP)，就在图像分类数据集上取得了很高的准确性。MLP-Mixer的一个重要亮点是，它包含两个主要的MLP层：一个独立应用于图像块（通道混合），另一个是层跨块应用（空间混合）。gMLP还表明，通过避免使用自注意和卷积（当前NLP和CV的实际使用的方式），可以在不同的图像识别和NLP任务中实现很高的准确性。读者显然不会使用MLP去获得最先进的性能，但它们与最先进的深度网络的可比性却是令人着迷的。再次使用卷积网络：2020年代的卷积网络自VisionTransformer（2020年）推出以来，计算机视觉的研究围绕着Transformer展开（在NLP领域，Transformer已经是一种规范）。VisionTransformer（ViT）&nbsp;在图像分类方面取得了最先进的结果，但在视觉下游任务（对象检测和分割）中效果不佳。随着SwinTransformers的推出，使得VisionTransformer很快也接管了视觉下游任务。很多人（包括我自己）都喜欢卷积神经网络。卷积神经网络确实能起效，而且放弃已经被证明有效的东西是很难的。这种对深度网络模型结构的热爱让一些杰出的科学家回到过去，研究如何使卷积神经网络（准确地说是ResNet）现代化，使其具有和VisionTransformer同样的吸引人的特征。特别是，他们探讨了「Transformers中的设计决策如何影响卷积神经网络的性能？」这个问题。他们想把那些塑造了Transformer的秘诀应用到ResNet上。MetaAI的SainingXie和他的同事们采用了他们在论文中明确陈述的路线图，最终形成了一个名为ConvNeXt的ConvNet架构。ConvNeXt在不同的基准测试中取得了可与SwinTransformer相媲美的结果。读者可以通过ModernConvNets库（现代CNN架构的总结和实现）了解更多关于他们采用的路线图。9结论深度学习是一个非常有活力、非常宽广的领域，很难概括其中所发生的一切。作者只触及了表面，论文多到一个人读不完，很难跟踪所有内容。例如，我们没有讨论强化学习和深度学习算法，如AlphaGo、蛋白质折叠AlphaFold（这是最大的科学突破之一）、深度学习框架的演变（如TensorFlow和PyTorch），以及深度学习硬件。或许，还有其他重要的事情构成了我们没有讨论过的深度学习历史、算法和应用程序的很大一部分。作为一个小小的免责声明，读者可能已经注意到，作者偏向于计算机视觉的深度学习，对其他专门为NLP设计的重要深度学习技术作者可能并没有涉及。此外，很难确切地知道某项特定技术是什么时候发表的，或者是谁最先发表的，因为大多数奇特的东西往往受到前人作品的启发。如有纰漏，读者可以去原文评论区与作者讨论。（原文链接：https://www.getrevue.co/profile/deeprevision/issues/a-revised-history-of-deep-learning-issue-1-1145664）本文转自微信公众号“人工智能科学与技术”。（完）更多精彩：校长专访｜扎根边疆民族地区聚焦师范教育主业培养高素质应用型人才——丽江师范高等专科学校陈本辉校长专访言十│计算机系统能力培养的回顾与前瞻“以学生为中心”教学的理念及落地路径探讨校长专访｜推动学科交叉融合培养新时代创新型人才——香港科技大学(广州)创校校长倪明选教授专访第七届编委会新年寄语计算学科课程思政教学指南陈国良院士｜计算机课程思政虚拟教研室文化建设南大陈道蓄教授｜变与不变：学习过程中的辩证法言十│关于高校青年教师的“困境”思考及建议徐晓飞等｜元宇宙教育及其服务生态体系【目录】《计算机教育》2024年第7期【目录】《计算机教育》2024年第6期【目录】《计算机教育》2024年第5期【目录】《计算机教育》2024年第4期【目录】《计算机教育》2024年第3期【编委寄语】北京大学李晓明教授：由“课堂教学改进元年”想到的……南大陈道蓄教授：教学生提问和教学生答问，哪个更重要？【言十系列】：计算机学科发展趋势及其对计算机教育的影响北大李晓明教授：从趣味数学到趣味算法到趣味编程——非专业学习者体会计算思维的一条途径？一流计算机学科建设的几个问题思考新工科与大数据专业建设他山之石可以攻玉——中外计算机教育研究文章汇编预览时标签不可点关闭更多小程序广告搜索「undefined」网络结果
关键词: 感知机, 卷积神经网络, Transformer视觉识别
AI技术: 感知机, 卷积神经网络（CNN）、循环神经网络（RNN）、长短期记忆网络（LSTM）、生成对抗网络（GAN）、Transformer。
行业: 计算机视觉, 自然语言处理, 机器学习
重大事件摘要: 这篇文章概述了深度学习从1958年至今的发展历程，重点介绍了各个时期的关键事件和突破。以下是文章中提到的重大事件：

1. **1958年**: 弗兰克·罗森布拉特发明了感知机，这是最简单的二元分类器，使用单位阶跃激活函数。

2. **1982-1986年**: 循环神经网络（RNN）被提出，用于处理序列数据，但面临长序列时的记忆问题。

3. **1998年**: LeNet-5架构被提出，是最早的卷积神经网络之一，用于文档识别。

4. **1998年**: 长短期记忆网络（LSTM）被提出，解决了RNN在处理长序列时的梯度不稳定问题。

5. **2002年**: AlexNet在ImageNet挑战赛中获胜，推动了深度卷积神经网络的发展。

6. **2012年**: AlexNet以15.3%的错误率赢得了ImageNet挑战赛，比之前的错误率几乎减半。

7. **2014年**: GoogLeNet（Inception-v1）被提出，通过引入Inception模块提高了网络的深度和宽度。

8. **2015年**: ResNet（残差网络）被提出，通过引入残差连接解决了深度网络训练中的退化问题。

9. **2014年**: 生成对抗网络（GAN）被Ian Goodfellow提出，用于生成新的数据样本。

10. **2017年**: Transformer被提出，基于注意力机制，彻底改变了NLP领域。

11. **2018年至今**: 深度学习算法、应用和技术突飞猛进，包括Vision Transformers、大规模语言模型（如GPT系列）、代码生成模型（如Codex和AlphaCode）等的发展。

这些事件标志着深度学习领域的重要里程碑，展示了从最初的简单模型到复杂的深度学习架构和算法的演变过程。
