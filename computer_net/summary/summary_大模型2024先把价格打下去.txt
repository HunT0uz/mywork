标题: 大模型2024:先把价格打下去
链接: https://baijiahao.baidu.com/s?id=1793227359312869898&wfr=spider&for=pc
总结: 图片来源@视觉中国文 | 惊蛰研究所，作者 | 雨谷AI新星OpenAI最近有点头疼，不仅公司和CEO被马斯克起诉，其拳头产品GPT-4在性能和价格上均面临竞争对手的冲击。近期，成立不到一年的法国人工智能创企Mistral AI发布了最新大模型Mistral Large，并推出了首个聊天机器人产品Le Chat，直接对标ChatGPT。据了解，Mistral Large在目前所有能通过API访问的大模型中评分第二，仅次于GPT-4。更值得关注的是，Mistral AI还与微软达成了更加深入的合作协议，微软将投资入股Mistral AI，并为其提供算力和云服务，而Mistral AI的大模型资源也将在微软的Azure云平台中售卖。要知道，上一个有此待遇的AI创业公司还是OpenAI。除此之外，更低廉的API接口价格也让Mistral Large成为了GPT-4的有力竞争者，并有望在当前的大模型军备竞赛中掀起一场价格战。作为一款诞生于欧洲的大模型，Mistral Large支持英语、法语、西班牙语、德语和意大利语，可深度理解语法和文化背景。另外，Mistral Large的上下文窗口为32K，可从约2.4万个英文单词的大型文档中精准提取信息；具备精确的指令跟随能力，便于开发者定制审核策略；支持原生函数调用和限定输出模式，助力应用开发规模化和技术栈现代化。性能方面，虽然Mistral AI并未公布Mistral Large的参数量，但其关键性能已达到业界前三。具体来看，Mistral Large在MMLU基准测试中的常识和推理得分为81.2%，仅次于GPT-4的86.4%。Mistral Large达到了顶级的推理能力，可用于复杂的多语言推理任务，包括文本理解、转换和代码生成。其推理准确性优于Anthropic的Claude 2、谷歌的Gemini 1.0 Pro、OpenAI的GPT-3.5，推理速度甚至超过了GPT-4和Gemini Pro，显示了其在处理复杂任务时的高效能力。多语言能力测试中，Mistral Large在法语、德语、西班牙语和意大利语的Arc Challenge、HellaSwag、MMLU等基准测试中的表现均远超目前公认最强的开源大模型——Meta的LLaMA 2 70B。数学和编程能力方面，Mistral Large同样表现不俗：其在MBPP基准测试中的编程得分高于LLaMA 2 70B，在Math maj@4基准测试中的数学得分也领先于GPT-3.5、Gemini Pro 1.0等模型。作为Mistral AI商用系列中的旗舰模型，Mistral Large与GPT-4一样并未开源。用户可通过三种方式访问与使用Mistral模型：其中，在欧洲的Mistral Al基础设施上安全托管的La Plateforme是开发者访问Mistral Al所有模型的首选方式，开发者可通过点击创建自己的应用程序和服务；Mistral Al的开源模型目前可通过GCP、AWS、Azure、NVIDIA等云服务商获得，而Mistral Large目前仅通过Azure云平台提供服务，包括Azure AI Studio和Azure Machine Learning。此外，开发者还可以通过虚拟云或on-prem自行部署使用Mistral模型，这种方式提供了更高级的自定义和控制，自有数据将保留在公司内部。价格方面，目前上下文窗口为128k的GPT-4 Turbo的输入价格为0.01美元/1000 token，输出价格为0.03美元/1000 token。相比之下，Mistral Large的输入、输出价格均为前者的80%。体验方面，有AI创业者指出，Mistral Large的使用体验碾压曾经的第三名Claude 2。截至2023年11月，OpenAI的开发者规模达200万，其中包含92%的世界500强企业。而Mistral Large直逼GPT-4的性能和更低的售价有望为需求量巨大的企业用户节省一大笔开支，从被OpenAI垄断的MaaS（模型即服务）市场撕开一个口子。Mistral Large把价格打下来的底气是更低的训练成本。OpenAI CEO Sam Altman曾表示，GPT-4的模型训练成本“远远超过了”5000万至1亿美元。而据Mistral AI创始人Arthur Mensch透露，Mistral Large的训练成本不到2200万美元，约为GPT-4的五分之一。除了真金白银的训练成本，后来者居上的Mistral Large的时间成本也更具优势。OpenAI从成立到推出GPT-4，足足用了8年，而Mistral AI推出仅次于GPT-4的Mistral Large只用了9个月。Mistral AI号称欧洲版OpenAI，创始团队由Meta和Deepmind的前科学家们组成。成立后的半年多时间里，Mistral AI接连完成1.05亿欧元种子轮融资和后续的4.15亿欧元融资，得到美国光速、a16z等顶级VC以及英伟达、赛富时、法巴银行的青睐。同期，Mistral AI先后推出号称当时“最强的70亿参数开源模型”Mistral 7B、首个开源MoE大模型Mistral 8x7B。其中，Mistral 8x7B更是以一条简单粗暴的磁力链接引领了大模型发布的新范式，给业界带来震撼。凭借巨额融资叠加新品发布，Mistral AI的估值也曾一夜之间飙升至20亿美元，成为大模型领域的新晋独角兽。而Mistral AI更引人关注的是，从初期只有6人的小团队成长至今，Mistral AI一直是MoE路线的忠实信徒。MoE即“混合专家模型”，这种模型设计策略通过将大模型分解为多个子模块，提高模型的容量、处理能力和效率。MoE架构主要由“专家”和门控机制两部分构成。每个“专家”相当于一个小型的Transformer模型，专门处理特定类型的输入数据，多个“专家”的结合则使模型具备了更好的性能。而门控机制则用于判定输入样本需由哪些“专家”接管处理。大模型的大规模应用与其算力成本紧密相关。对于模型厂商而言，目前主要的算力成本包括预训练成本和推理成本。除去GPU每秒运算次数和显卡的租用成本这两个常量后，大模型的预训练成本与模型参数量和训练数据的token量正相关，推理成本与模型参数量正相关。而大模型的性能通常与其参数量相关联，而越高的参数量意味着越高的算力成本。因此，如何在同样的算力成本下提升大模型的参数量成了破局的关键。而MoE的解题思路是引入稀疏性，即模型训练过程中，各有所长的“专家”们独立训练、各司其职，在过滤重复信息、减少数据干扰的同时大幅提升模型的学习速度与泛化能力；在推理过程中，每次推理只按需调用部分“专家”，激活其对应的部分参数，如此便有效降低了相同参数下大模型的算力成本。有意思的是，OpenAI在去年成为“当红炸子鸡”成功得到众多重度用户的续费后，被曝采用MOE重新设计了GPT-4构架，导致性能受到影响。尽管OpenAI官方并未对此进行正面回应，但利用MOE架构降低训练成本，已经被认为是一个无比自然的发展方向。Mistral AI同样未公布大模型的具体参数与训练数据Token数，但此前谷歌应用MoE开发出的GLaM模型参数量达12000亿、训练数据16000亿token，分别是GPT-3.5的6.8倍和5.3倍，其实际的训练成本却只有GPT-3.5的三分之一也印证了MoE框架的高效。延续着MoE的路线，如果说此前发布的开源模型Mistral 7B、Mistral 8x7B实现了对LLaMA等大参数开源模型的逆袭，此次发布的Mistral Large则是Mistral AI对可持续商业模式的探索，试图以闭源模型搭建可盈利的产品线。顶着对华芯片禁售的压力，芯片巨头英伟达以一份耀眼的四季报打消了市场顾虑：在数据中心与游戏业务双核驱动下，英伟达2023年四季度营收、净利润大幅超出预期，毛利率再创历史新高。业绩加持下，英伟达业绩已突破2万亿美元，更接连超越亚马逊、沙特阿美，成为仅次于微软和苹果的全球第三大公司。数据、算力和算法构成了大模型的基石。在当下这波如火如荼的大模型淘金热中，从学界到初创企业再到巨头纷纷下场，而无论其技术路线是开源或闭源，应用场景是通用或垂直，AI芯片作为大模型大脑，始终是模型预训练和推理必不可少的工具。身为高端GPU市场中唯一的提供方，“军火商”英伟达是这场大模型军备竞赛中永远的赢家——以A100为例，若要通过训练达到ChatGPT级别的性能，至少消耗一万张A100加速卡，巨头们囤货的单位也以万张起，怎能不赚得盆满钵满？但换个角度来看，在GPU供应短缺的背景下，一张A100显卡售价约10000美元甚至更高，对于大模型厂商来说，在应用落地和商业化前景仍不明朗的情况下，动辄上亿美元真金白银的投入必然肉疼。在算力、数据、人力等资源成本高企的情况下，如何用相对低的成本训练出一个想要的大模型，并以一个用户可接受的成本让大模型跑起来是大模型行业在2024年的当务之急。在保证同等效果前提下，提高硬件利用率，缩短算力使用时长；优化工具链以提高训练、推理效率；适配低价GPU是当前国内大模型厂商降本的主流方法论。例如，面向大模型训练，腾讯升级了自研机器学习框架Angel，针对预训练、模型精调和强化学习等全流程进行了加速和优化，提升了内存的利用率。借此，大模型训练效率可提升至主流开源框架的2.6倍，用该框架训练千亿级大模型可节省50%算力成本，大模型推理速度提高了1.3倍。京东云推出vGPU池化方案，提供一站式GPU算力池化能力，结合算力的任意切分和按需分配，在同等GPU数量的前提下，实现了数倍业务量扩展和资源共享，降低了硬件采购成本，使用更少的AI芯片支撑了更多的训练和推理任务，GPU利用率最高提升70%，大幅降低大模型推理成本。阿里云通义大模型则聚焦于规模定理，基于小模型数据分布、规则和配比，研究大规模参数下如何提升模型能力，并通过对底层集群的优化，将模型训练效率提升了30%，训练稳定性提升了15%。百度升级了异构计算平台“百舸”，将训练和推理场景的吞吐量提高了30%-60%，意味着原先需要用100天的训练才能达成的效果，现在只需40-70天，节约时间等于间接省钱。同时，在英伟达之外，百度的“千帆”大模型平台还兼容昆仑芯、昇腾、海光DCU、英特尔等国内外其他主流AI芯片，通过组合选项完成低成本的算力适配。正所谓“早买早享受，晚买有折扣。”当前，Mistral AI以性价比暂时领先，但也有不少开发者还在等待OpenAI大模型产品的升级降价。毕竟，正是OpenAI自己在GPT-4发布后不到8个月就推出了更强也更便宜的GPT-4 Turbo。举报/反馈
关键词: Mistral Large,  GPT-4,  MoE架构
AI技术: 混合专家模型（MoE）, 稀疏性, 时间成本
行业: 人工智能, 芯片制造, 云计算
重大事件摘要: 这篇文章中的重大事件包括：

1. 法国人工智能创企Mistral AI发布了最新的大模型Mistral Large，该模型在性能上仅次于GPT-4，并且推出了首个聊天机器人产品Le Chat，直接对标ChatGPT。
2. Mistral AI与微软达成了更深入的合作协议，微软投资入股并提供算力和云服务，Mistral Large的大模型资源也将在Azure云平台售卖。
3. Mistral Large支持英语、法语、西班牙语、德语和意大利语，能深度理解语法和文化背景，并具备上下文窗口为32K的能力。
4. Mistral Large的性能在MMLU基准测试中的常识和推理得分为81.2%，仅次于GPT-4的86.4%。其推理准确性优于Anthropic的Claude 2、谷歌的Gemini 1.0 Pro和OpenAI的GPT-3.5，且处理复杂任务时的效率高于GPT-4和Gemini Pro。
5. Mistral Large的数学和编程能力表现不俗，其在MBPP测试中的得分高于LLaMA 2 70B，并在Math@Maj@4基准测试中领先。
6. Mistral AI的创始人Arthur Mensch透露，Mistral Large的训练成本不到2200万美元，约为GPT-4的五分之一，且从成立到推出Mistral Large仅用了9个月时间。
7. Mistral AI采用MoE（混合专家模型）架构，这种模型设计策略通过将大模型分解为多个子模块来提高模型的容量、处理能力和效率，同时降低算力成本。
8. Mistral AI完成了多轮融资，估值曾飙升至20亿美元，成为大模型领域的新晋独角兽。
9. 英伟达作为高端GPU市场中唯一的提供方，在数据中心与游戏业务双核驱动下，营收和净利润大幅超出预期，成为全球第三大公司。
10. 国内大模型厂商正在通过提高硬件利用率、优化工具链和适配低价GPU等方法降低成本，以应对高昂的算力、数据和人力成本。
