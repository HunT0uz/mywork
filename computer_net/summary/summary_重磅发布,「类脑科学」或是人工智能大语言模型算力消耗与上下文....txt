标题: 重磅发布,「类脑科学」或是人工智能大语言模型算力消耗与上下文...
链接: https://baijiahao.baidu.com/s?id=1780264005701746514&wfr=spider&for=pc
总结: 在一场科幻与科学的盛会上，科幻突然照进了现实。近日，在深圳先进院，深圳理工大学教育基金会和科学与幻想成长基金开办了一场以科幻和AI涌现为背景的活动。一个来自深圳名为陆兮科技的团队，首次公开发布了他们的人工智能大语言模型---NLM(Neuromorphic Generative Pre-trained Language Model)，一个不基于Transformer的大语言模型。与国内外众多的大模型不同的是，该团队以类脑科学与类脑智能为核心，同时融合循环神经网络的特性，以大脑的高效运算特性为启发开发大语言模型。更惊人的是，该模型在同级别参数下的算力消耗为Transformer架构的1/22；在上下文长度问题上，NLM也交出了满分答卷：上下文长度窗口可以实现无限增长，无论是开源LLM的2k限制，又或是其他32k，100k的上下文长度限制都不在话下。什么是类脑计算？类脑计算是一种模仿人脑结构和功能的计算模式，它在架构、设计原理和信息处理方式上模拟人脑的神经网络连接。这种计算不仅仅是简单地尝试模拟生物神经网络的表面特征，而是深入到如何模拟生物神经网络的基本构造方式——即通过神经元和突触的大规模互联来处理和存储序列信息。与传统的基于规则的算法不同，类脑计算依赖于大量的互联神经网络自主学习和提取信息，就像人类大脑一样。这种方法允许计算系统从经验中学习、适应新情况、理解复杂的模式，并能进行高级决策和预测。由于其高度的自适应性和并行处理能力，类脑计算系统在处理大数据、图像和语音识别、自然语言处理等领域表现出了极高的效率和准确性。这些系统不仅能快速处理复杂多变的信息，而且在能耗和计算资源上远远低于传统的计算架构，因为它们不需要进行大量的预先编程和数据输入。总的来说，类脑计算开辟了一种全新的计算范式。它超越了传统的人工神经网络，向着能够自我学习、自我组织、甚至具有一定自我意识的高级智能系统迈进。类脑大模型的先进性在活动上，陆兮团队的周芃博士详细解释了类脑大模型的实现机理。作为新一代的神经网络模型，又称类脑神经网络，它突破了前两代神经网络的缺点。-第一代神经网络（又称：MLP多层感知机），在传输信号上为0和1，无法处理过于复杂的任务，算力开销也不多。-第二代神经网络，也就是人工神经网络，将传输信号变为了[0-1]的连续区间，有了足够的复杂度，但是算力开销也飙升。- 第三代神经网络，也就是类脑神经网络，将信号变成了脉冲序列，拥有足够复杂度的同时，也将算力开销变得可控。这种脉冲序列是模仿神经结构中的动力学特性得以实现的。同时，序列意味着时间，第三代神经网络可以有效的将信息中的时间信息融合和输出。-相较于前两代神经网络，它更有效地处理有时间维度的序列信息，更有效地认识真实世界。基于类脑算法的大模型，在推理时的原理也和Transformer迥然不同。在推理过程中，Transformer模型和类脑模型的运作机制存在显著差异。每当Transformer模型进行推理时，其将会综合考虑全部的上下文信息以产生下一个token。这一操作可以类比为在聊天过程中，每当我们说出一个字时，都需要回想当天所有的经历。这也是目前大型模型在参数持续增长的同时，其计算成本随之上升的主要原因。相对而言，类脑模型在推理时只需要依赖其内部状态和一个token。这可以比喻为，我们在说话时脱口而出下一个字是什么，而不必具体回忆之前的所有情境，说话内容也与之前的经历有内在关联。此种机制是NLM能大幅减少算力开销的关键，使其能更接近人类大脑的运作方式，并因此显著提高了其性能。同样因为脑启发的特性，上下文长度有限也不再是令人困扰的问题。使用第三代神经网络的NLM大模型由于处理下一个token时需要的算力并不跟上下文长度相关，所以不存在上下文长度的瓶颈。公开可用的Transformer架构的大语言模型上下文长度仅为100k，向上增加上下文长度不仅仅是算力开销的问题，更是“能不能”的问题。NLM无限长度的上下文将可以打开大语言模型应用的想象力之门，无论是研究复杂的财报、阅读数十万字的小说，亦或是通过长度无限的上下文来使大模型“更懂你”，都可以成为现实。陆兮团队眼中的AI在这次活动上，陆兮科技的创始人CTO周芃博士阐释了团队在当下的使命-赋智万物。一个人工智能时代，需要人工智能处处普及，正如互联网和电力已经在我们的身边处处可见。目前的人工智能在能力上虽然令人惊叹，但其运行成本却为企业和消费者带来巨大负担。绝大多数手机、手表、平板和笔记本电脑在当前技术下无法完整、系统、高效、高质量地运行生成式人工智能大语言模型，研发大模型应用的门槛也让许多有志于此的优秀开发者望而却步。在活动现场，陆兮科技向观众展示了如何在一台普通安卓手机的离线模式下使用「NLM-GPT」大模型完成工作与生活中常见的各项任务，将活动推向了高潮。- 参与演示的手机搭载了市面上常见的芯片架构，与C端市场常见安卓机型的性能相仿。在手机处于飞行模式、未连接网络的前提下，陆兮科技展示了「NLM- GPT」大模型在这台手机上与用户实时对话、回答用户提出的问题、完成包括诗词创作、菜谱撰写、知识检索、文件解读等复杂程度高、对手机硬件性能参数要求较高、传统意义上需要联网才能完成的各项指令。- 整个演示过程中，手机的能耗平稳，对正常待机时长影响极低，对手机整体使用性能未造成任何影响。-该演示成功证明了「NLM-GPT」大模型拥有在诸如智能手机、平板电脑等小型C端商用设备中全场景、高效率、低功耗、零流量消耗运行的潜力。这意味着，得益于「NLM-GPT」大模型的赋能，手机、手表、平板、笔记本电脑等设备可以更加准确高效地理解人类的真实意图，在办公、学习、社交、娱乐等各类应用场景下更高质量地完成人类提出的各项指令与任务，极大地提升社会生产和人类生活的效率与质量。陆兮科技认为，「类脑科技」驱动的「生成式人工智能大语言模型」将全面拓展人类在学习、工作和生活等各个领域的思维力、感知力和行动力，提升全人类的整体智慧。得益于类脑科技的赋能，人工智能将不再是替代人类的新智能体，而是会成为人类改变世界、创造更加美好未来的高效智能工具。正如古人训练了猎犬和猎鹰，但猎人这一职业并不会因为猎犬和猎鹰的出现而消失。相反，猎人因此获益，掌握了猎犬和猎鹰所拥有的、人类自身并不具有的力量，更高效地获取猎物、为人类族群的壮大和人类文明的发展提供了动力和养分。在未来，在日常工作生活中应用人工智能大语言模型将不再是一项复杂的多流程系统工程，而是将如同「结账时打开付款码」、「拍照时按下快门」、「刷短视频时一键三连」一般简单、自然、流畅。陆兮团队将会持续耕耘于类脑计算领域，深入研究大脑这个大自然送给人类最宝贵的礼物，将类脑智能带入日常生活。也许，在不久的将来，人类会拥有更多的人工智能新伙伴。他们身体里不流淌血液，他们的智慧不会取代人类。在类脑技术的加持下，他们将会与我们一起，共同探索宇宙的奥秘，拓宽社会的边界，创造更加美好的未来。来源：生活报（来源：undefined）更多精彩资讯请在应用市场下载“极目新闻”客户端，未经授权请勿转载，欢迎提供新闻线索，一经采纳即付报酬。举报/反馈
关键词: 类脑科学,  人工智能大语言模型,  NLM
AI技术: 类脑科学,  循环神经网络,  脉冲序列
行业: 人工智能,  类脑计算,  大语言模型
重大事件摘要: 这篇文章报道了深圳陆兮科技团队在深圳先进院举办的一场科幻与AI活动中，首次公开发布了他们的人工智能大语言模型——NLM（Neuromorphic Generative Pre-trained Language Model）。该模型不基于Transformer架构，而是以类脑科学与类脑智能为核心，融合循环神经网络特性，模拟大脑的高效运算特性。NLM在同级别参数下的算力消耗仅为Transformer架构的1/22，且上下文长度窗口可以实现无限增长。

陆兮团队的周芃博士详细解释了类脑大模型的实现机理，包括其如何突破前两代神经网络的缺点，以及在推理过程中与Transformer模型的显著差异。此外，文章还展示了陆兮科技如何在一台普通安卓手机的离线模式下使用「NLM-GPT」大模型完成各种任务，证明了其在小型C端商用设备中全场景、高效率、低功耗、零流量消耗运行的潜力。

最后，文章强调了类脑科技将全面拓展人类在学习、工作和生活等各个领域的思维力、感知力和行动力，提升全人类的整体智慧。得益于类脑科技的赋能，人工智能将成为人类改变世界、创造更加美好未来的高效智能工具。
