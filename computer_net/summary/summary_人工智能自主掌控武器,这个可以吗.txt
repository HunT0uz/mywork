标题: 人工智能自主掌控武器,这个可以吗?
链接: https://news.cctv.com/m/a/index.shtml?id=ARTIWZks6uWVqPFWBD1dHRL7180412
总结: 
原标题：人工智能自主掌控武器，这个可以吗？——韩国研发自主武器引发业界深忧
						　　记者 张 晔　　最近，韩国科学技术院大学与军工企业“韩华 Systems”合作研发自主武器，遭全球50多名人工智能学者联名抵制，南京大学周志华教授是唯一一名中国内地学者。　　周志华认为，其危险之处在于让机器自己决定是否对人类进行毁灭性打击，“自主武器首先在伦理上就是错误的”。　　近年来，人工智能技术突飞猛进，产业界为此狂热不已。但是从已经暴露出的多起舆论事件来看，人工智能技术正在被利益绑架，伦理规范、法律约束一片苍白，这已引发业界的深忧。　　自主武器要举起简单规则的屠刀　　兵者不祥之器，非君子之器，不得已而用之。我国著名思想家老子对武器的使用曾有恰如其分的阐述。　　“武器作为一种必要的‘恶’而存在，它的伦理规范要高得多。”东南大学程国斌副教授认为，与人类控制的武器相比，自主武器依靠提前设定的程序，但再复杂的程序也是一种简单规则，一旦达到触发条件，即可自动执行，比如对方携带武器或有显著特征等，“而现场的环境是极其复杂多变的，换成是人类，当时的决策也不见得就正确，更何况是机器依据一个简单的规则”。　　一般的智能武器早就有了，但是最终做决定的都是人类。而自主武器，是要把这个“决定权”交给机器，让机器去决定是否对人类进行杀戮。　　“如果这样的武器开发出来，自主武器将导致战争的第三次革命，战争将比以往任何时候都更容易发生并且更残忍。”周志华等50多名学者在公开信中写道，它们有可能成为恐怖分子的武器。暴君和恐怖分子可以利用它们对付无辜的人群，他们不会理会任何的道德限制。如果打开了这个潘多拉魔盒，将很难关闭。　　任何一个科研领域都存在不该去触碰的东西。例如克隆人是被主流生命科学界所禁止的。这就是科技伦理划定的禁区，也是主流价值观和人类道德的共识。　　“人类在使用武器时，受自由意志和道德责任支配，而自动武器则是依据固定程序做出判断，既没有责任主体，也不会产生对杀戮的道德反思，这非常可怕。”程国斌说。　　两种伦理规范缺一不可　　设想一下：当一辆自动驾驶汽车在高速行驶，前方车道上突然跑出一个人，如果保持车道不打方向就有可能撞死对方保住自己，如果急打方向避开对方却有可能丢了自己的性命……这时，计算机如何决策才是合理的？　　这是人工智能业界广为讨论的一个伦理问题，就如同妈妈和女友落水先救谁的问题一样，答案并不明朗。　　“人工智能面对的伦理问题，其实都是传统伦理学讨论过的。只是问题形式变了，从而引起公众关注。”程国斌告诉记者，把这个案例中的驾驶者换成人类，就不会引起这么广泛的关注。　　学界把人工智能伦理分为两方面，一是对机器而言，人类设计的程序，本身就包含道德规范在其中；另一方面是对人，将技术或产品应用的人群，也必须受道德约束。两者缺一不可。　　周志华认为，强人工智能“不能做、不该做！”强人工智能即具有心智和意识、能根据自己的意图开展行动。霍金、马斯克等担忧的“人工智能威胁人类”，即指强人工智能。迄今为止，主流研究都不支持让机器拥有自主意识。　　而对于技术使用者的伦理和法规要求，目前还显得支离破碎和虚弱无力。例如，近日支付宝因收集个人金融信息不符合最少、必须原则，且信息使用不当被罚5万元。有网友戏称，“这处罚也就是罚酒一杯”。　　“在网络信息领域，技术走得太快，伦理却没有共识，比如数据搜集技术非常高效，但是几乎没有有效的控制和评价的手段，这就像一个小孩拿枪玩耍，谁也不知道危险会在何时以什么样的方式发生。”程国斌说。　　伦理不是技术创新的绊脚石　　过去，伦理对计算机科学来说似乎不太相关。“码农”写的软件，似乎不大可能造成身体的伤害、疼痛或死亡。　　但是，近年来伦理问题陡增。国内的大数据杀熟舆论事件还未平息，国外就曝出Facebook的数据泄露为政治竞选服务的丑闻。技术带来的负面问题，极大地增加了公众的焦虑和不信任。　　2018年新学期，哈佛、康奈尔、MIT、斯坦福等美国高校的课程表上多了一门新课程，名为人工智能伦理、数据科学伦理、技术伦理、机器人伦理等。　　与此同时，谷歌、亚马逊、Facebook等越来越多的互联网和人工智能公司都开始拥抱科技伦理，甚至成立专门的伦理中心或伦理委员会，招聘人工智能政策和伦理研究员。　　而我国，才刚刚开始针对理工科研究生系统化地推行科技伦理教育，更毋庸说是详细地划分不同科学领域的伦理课程。　　“这是因为西方社会受宗教文化影响，始终对科技发展怀有警惕之心，对伦理问题更为敏感。”程国斌说，在我国还是对技术持简单乐观主义占主流。　　2016年3月，“阿尔法狗”横扫围棋世界冠军，让人工智能名声大噪。“产业界、金融界、技术界一下子狂热起来，但是伦理和法律确实没跟上。”南京信息工程大学教授徐军说，“技术是一把双刃剑，就看是谁在用它。”　　技术发展不同于科学探索，它指向更加具体的目标。近代德国社会学家马克斯·韦伯曾提出工具理性与价值理性区分的学术概念，其核心是说技术或者工具追求的是如何高效地实现既定的目标，而价值理性的任务是考察这一目标对人类福祉和社会发展所具有的价值，两者必须统一。　　“当出现伦理问题，单靠几十名学者呼吁是不够的，需要开发者、决策者以及社会各方形成共识。”徐军认为。　　但是，在道德观愈加多元化的今天，形成社会共识越来越难。即使如此，专家们还是呼吁，人工智能的开发者、决策者应该更加自律；相关企业应设立伦理审查机制，以防止技术被滥用；相关高校应尽快开设人工智能伦理课程，让学生走出校门可以做到负责任地创新；同时，要设定更加严厉的法律法规红线，大幅提高违法成本。
关键词: 人工智能,  自主武器,  伦理问题
AI技术: 人工智能自主掌控武器，自动驾驶汽车，大数据杀熟
行业: 军工企业、互联网和人工智能公司、金融界
重大事件摘要: 这篇文章讨论了韩国科学技术院大学与军工企业“韩华 Systems”合作研发自主武器的事件，这一事件引发了全球50多名人工智能学者的联名抵制，其中包括南京大学的周志华教授。文章指出，自主武器的开发在伦理上存在严重问题，因为它让机器自己决定是否对人类进行毁灭性打击。此外，文章还提到了人工智能技术被利益绑架、伦理规范和法律约束不足的问题，以及自主武器可能导致战争的第三次革命，使战争更容易发生且更残忍。专家们呼吁开发者、决策者和社会各方形成共识，设立伦理审查机制，防止技术被滥用，并设定更加严厉的法律法规红线。
