标题: 【升华】人工智能学习之深度学习概述-CSDN博客
链接: http://www.baidu.com/link?url=VgLXbE6KtcjPzuNhGvIPavcGiVwyUIQiy6Gk8wThOgKWP4D5Mi10AhZ1ipI5fhAfWmuOcEBdjRQAYelsnDzckIVLLG7fROFMYfpEXMi_5-m
总结: 一、深度学习概述深度学习是机器学习的一个分支领域，强调从一系列连续的表示层中学习。现代的深度学习模型通常包含数十个甚至上百个连续的表示层，它们都是从训练数据中自动学习而来。与之对应，机器学习有时也被称为浅层学习。在深度学习中，这些分层表示是通过叫作神经网络的模型学习得到的。深度神经网络可以看作多级信息蒸馏过程：信息穿过连续的过滤器，其纯度越来越高。机器学习发展过程技术定义：一种多层的学习数据表示的方法。二、深度学习的核心概念神经网络：深度学习的基本构建单元是人工神经网络。神经网络由大量的节点（或称为神经元）组成，这些节点通过连接（权重）相互作用。神经网络的层次结构使得它能够从数据中学习到越来越复杂的特征。多层结构：深度学习的“深度”指的是神经网络的层数。网络的每一层提取的数据特征不同，深层网络能够学习到更高层次的抽象特征。例如，在图像识别任务中，低层可能学习到边缘信息，中层可能学习到纹理，高层可能学习到物体的形状和结构。端到端学习：深度学习通常采用端到端的学习方式，即从原始数据直接输入模型，模型自动学习特征并进行预测，而不需要人工设计复杂的特征提取步骤。&nbsp;三、深度学习的基本原理a.对神经网络的权重（有时也被称为该层的参数）进行随机赋值b.经过一系列随机变换，得到预测值Y'c.通过损失函数（有时也被称为目标函数或代价函数），得到预测值Y'与真实值Y之间的损失值d.将损失值作为反馈信号，通过优化器来对权重值进行微调，以降低当前示例对应的损失值e.循环重复足够做的次数（b-d），得到具有最小损失值的神经网络，就是一个训练好的神经网络&nbsp;四、深度学习神经网络介绍人工神经网络的基本结构人工神经网络（ANN）是深度学习的核心构建块。其基本结构可以分为以下几部分：输入层：输入层是神经网络的第一层，用于接收输入数据。每个神经元（节点）对应数据的一个特征。例如，在图像分类任务中，每个输入神经元可以对应图像中的一个像素值。隐藏层：隐藏层位于输入层和输出层之间，用于学习数据的特征。每个隐藏层的神经元将接收来自前一层的输出，经过加权求和和激活函数处理后，再传递给下一层。深度学习中的"深度"指的就是隐藏层的数量。隐藏层的每一层都能够提取数据的不同层次特征。输出层：输出层生成模型的最终输出。对于分类任务，输出层的神经元数量通常等于类别数，每个神经元输出一个类别的概率。对于回归任务，输出层可以是一个或多个连续值。激活函数、损失函数与优化算法激活函数：激活函数决定了神经元的输出，它引入了非线性变换，使得神经网络能够学习复杂的模式。常见的激活函数包括：Sigmoid：输出值在0到1之间，适用于二分类问题。公式为(\sigma(x)=\frac{1}{1+e^{-x}})。ReLU（RectifiedLinearUnit）：将负值输出为0，正值保持不变，计算简单且效果良好。公式为(\text{ReLU}(x)=\max(0,x))。Tanh：将输出值映射到-1到1之间，比Sigmoid具有更强的非线性。公式为(\tanh(x)=\frac{e^x-e{-x}}{ex+e^{-x}})。损失函数：损失函数度量预测值与实际值之间的差异。在训练过程中，目标是最小化损失函数。常见的损失函数有：均方误差（MSE）：用于回归任务，计算预测值与实际值之间的平均平方差。公式为(\text{MSE}=\frac{1}{n}\sum_{i=1}^{n}(y_i-\hat{y}_i)^2)。交叉熵损失（Cross-EntropyLoss）：用于分类任务，计算真实标签与预测概率分布之间的差异。公式为(L=-\sum_{i=1}^{n}y_i\log(\hat{y}_i))。优化算法：优化算法用于调整网络的权重，使得损失函数最小化。常见的优化算法包括：梯度下降（GradientDescent）：通过计算损失函数关于权重的梯度来更新权重，更新公式为(w:=w-\eta\frac{\partialL}{\partialw})，其中(\eta)是学习率。随机梯度下降（SGD）：每次更新仅使用一个或少量样本计算梯度，适合处理大规模数据。Adam（AdaptiveMomentEstimation）：结合了动量法和RMSProp，动态调整学习率。公式为(\text{Adam}=\text{LearningRate}\times\frac{\text{Gradient}}{\sqrt{\text{Variance}+\text{Epsilon}}})。&nbsp;神经网络的数据表示目前所有机器学习系统都使用张量（tensor）作为基本数据结构，张量对这个领域非常重要，TensorFlow就是以它来命名。张量这一概念的核心在于，它是一个数据容器。它包含的数据通常是数值数据，因此它是一个数字容器。你可能对矩阵很熟悉，它是2阶张量。张量是矩阵向任意维度的推广，张量的维度通常叫做轴。张量是由以下3个关键属性来定义的。•轴：轴的个数•形状：表示张量沿每个轴的维度大小（元素个数）•数据类型（dtype）：数据的类型，可以是float16、float32、float64、unit8、string等五、主要的深度学习框架&nbsp;文章知识点与官方知识档案匹配，可进一步学习相关知识Python入门技能树人工智能深度学习462595人正在系统学习中
关键词: 深度学习, 神经网络, 激活函数, 损失函数
AI技术: 深度学习, 机器学习, 神经网络, 张量, 优化算法
行业: 机器学习，深度学习，神经网络
重大事件摘要: 这篇文章主要介绍了深度学习的概述、核心概念、基本原理、神经网络的结构以及主要的深度学习框架。以下是文章的重大事件总结：

1. **深度学习概述**：深度学习是机器学习的一个分支，强调从一系列连续的表示层中学习。现代的深度学习模型通常包含数十个甚至上百个连续的表示层，它们都是从训练数据中自动学习而来。

2. **深度学习的核心概念**：
   - **神经网络**：深度学习的基本构建单元是人工神经网络，由大量的节点（或称为神经元）组成，这些节点通过连接（权重）相互作用。
   - **多层结构**：深度学习的“深度”指的是神经网络的层数。网络的每一层提取的数据特征不同，深层网络能够学习到更高层次的抽象特征。
   - **端到端学习**：深度学习通常采用端到端的学习方式，即从原始数据直接输入模型，模型自动学习特征并进行预测，而不需要人工设计复杂的特征提取步骤。

3. **深度学习的基本原理**：
   - 对神经网络的权重进行随机赋值。
   - 经过一系列随机变换，得到预测值Y'。
   - 通过损失函数得到预测值Y'与真实值Y之间的损失值。
   - 将损失值作为反馈信号，通过优化器来对权重值进行微调，以降低当前示例对应的损失值。
   - 循环重复上述过程足够多的次数，得到具有最小损失值的神经网络，就是一个训练好的神经网络。

4. **神经网络的结构**：
   - **输入层**：用于接收输入数据。
   - **隐藏层**：位于输入层和输出层之间，用于学习数据的特征。
   - **输出层**：生成模型的最终输出。

5. **激活函数、损失函数与优化算法**：
   - **激活函数**：决定了神经元的输出，引入非线性变换，使得神经网络能够学习复杂的模式。常见的激活函数包括Sigmoid、ReLU（Rectified Linear Unit）和Tanh。
   - **损失函数**：度量预测值与实际值之间的差异。常见的损失函数有均方误差（MSE）和交叉熵损失（Cross-Entropy Loss）。
   - **优化算法**：用于调整网络的权重，使得损失函数最小化。常见的优化算法包括梯度下降（Gradient Descent）、随机梯度下降（SGD）和Adam（Adaptive Moment Estimation）。

6. **神经网络的数据表示**：目前所有机器学习系统都使用张量（tensor）作为基本数据结构。张量是一个数据容器，包含的数据通常是数值数据。张量是由轴、形状和数据类型这三个关键属性来定义的。

7. **主要的深度学习框架**：文章提到了一些主要的深度学习框架，但未具体列出名称。
