标题: 深度学习概述_公路构造深度学习内容-CSDN博客
链接: http://www.baidu.com/link?url=yylqSBxXAMO497yIJlng5sTn2CmqE-JsuL05qqL1g1jArNdf_kMZSbewVkcg8uu0E8mMdzoDK6iUIVNrScXf8sa5YFdNRRjBGeHbaj9Rag_
总结: 这篇文章主要是为了对深度学习(DeepLearning)有个初步了解，算是一个科普文吧，文章中去除了复杂的公式和图表，主要内容包括深度学习概念、国内外研究现状、深度学习模型结构、深度学习训练算法、深度学习的优点、深度学习已有的应用、深度学习存在的问题及未来研究方向、深度学习开源软件。一、&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;深度学习概念深度学习(DeepLearning,DL)由Hinton等人于2006年提出，是机器学习(MachineLearning,ML)的一个新领域。深度学习被引入机器学习使其更接近于最初的目标----人工智能（AI，ArtificialIntelligence）。深度学习是学习样本数据的内在规律和表示层次，这些学习过程中获得的信息对诸如文字、图像和声音等数据的解释有很大的帮助。它的最终目标是让机器能够像人一样具有分析学习能力，能够识别文字、图像和声音等数据。深度学习是一个复杂的机器学习算法，在语言和图像识别方面取得的效果，远远超过先前相关技术。它在搜索技术、数据挖掘、机器学习、机器翻译、自然语言处理、多媒体学习、语音、推荐和个性化技术，以及其它相关领域都取得了很多成果。深度学习使机器模仿视听和思考等人类的活动，解决了很多复杂的模式识别难题，使得人工智能相关技术取得了很大进步。2006年，机器学习大师、多伦多大学教授GeoffreyHinton及其学生Ruslan发表在世界顶级学术期刊《科学》上的一篇论文引发了深度学习在研究领域和应用领域的发展热潮。这篇文献提出了两个主要观点：（1）、多层人工神经网络模型有很强的特征学习能力，深度学习模型学习得到的特征数据对原数据有更本质的代表性，这将大大便于分类和可视化问题；（2）、对于深度神经网络很难训练达到最优的问题，可以采用逐层训练方法解决。将上层训练好的结果作为下层训练过程中的初始化参数。在这一文献中深度模型的训练过程中逐层初始化采用无监督学习方式。2010年，深度学习项目首次获得来自美国国防部门DARPA计划的资助，参与方有美国NEC研究院、纽约大学和斯坦福大学。自2011年起，谷歌和微软研究院的语音识别方向研究专家先后采用深度神经网络技术将语音识别的错误率降低20%-30%，这是长期以来语音识别研究领域取得的重大突破。2012年，深度神经网络在图像识别应用方面也获得重大进展，在ImageNet评测问题中将原来的错误率降低了9%。同年，制药公司将深度神经网络应用于药物活性预测问题取得世界范围内最好结果。2012年6月，AndrewNG带领的科学家们在谷歌神秘的X实验室创建了一个有16000个处理器的大规模神经网络，包含数十亿个网络节点，让这个神经网络处理大量随机选择的视频片段。经过充分的训练以后，机器系统开始学会自动识别猫的图像。这是深度学习领域最著名的案例之一，引起各界极大的关注。深度学习本质上是构建含有多隐层的机器学习架构模型，通过大规模数据进行训练，得到大量更具代表性的特征信息。从而对样本进行分类和预测，提高分类和预测的精度。这个过程是通过深度学习模型的手段达到特征学习的目的。深度学习模型和传统浅层学习模型的区别在于：（1）、深度学习模型结构含有更多的层次，包含隐层节点的层数通常在5层以上，有时甚至包含多达10层以上的隐藏节点；（2）、明确强调了特征学习对于深度模型的重要性，即通过逐层特征提取，将数据样本在原空间的特征变换到一个新的特征空间来表示初始数据，这使得分类或预测问题更加容易实现。和人工设计的特征提取方法相比，利用深度模型学习得到的数据特征对大数据的丰富内在信息更有代表性。在统计机器学习领域，值得关注的问题是如何对输入样本进行特征空间的选择。例如对行人检测问题，需要寻找表现人体不同特点的特征向量。一般来说，当输入空间中的原始数据不能被直接分开时，则将其映射到一个线性可分的间接特征空间。而此间接空间通常可由3种方式获得：定义核函数映射到高维线性可分空间，如支持向量机（supportvectormachine，SVM）、手工编码或自动学习。前2种方式对专业知识要求很高，且耗费大量的计算资源，不适合高维输入空间。而第3种方式利用带多层非线性处理能力的深度学习结构进行自动学习，经实际验证被普遍认为具有重要意义与价值。深度学习结构相对于浅层学习结构[如SVM、人工神经网络（artificialneuralnetworks，ANN），能够用更少的参数逼近高度非线性函数。深度学习是机器学习领域一个新的研究方向，近年来在语音识别、计算机视觉等多类应用中取得突破性的进展。其动机在于建立模型模拟人类大脑的神经连接结构，在处理图像、声音和文本这些信号时，通过多个变换阶段分层对数据特征进行描述，进而给出数据的解释。以图像数据为例,灵长类的视觉系统中对这类信号的处理依次为：首先检测边缘、初始形状、然后再逐步形成更复杂的视觉形状，同样地，深度学习通过组合低层特征形成更加抽象的高层表示、属性类别或特征，给出数据的分层特征表示。深度学习之所以被称为"深度"，是相对支持向量机(supportvectormachine,SVM)、提升方法(boosting)、最大熵方法等"浅层学习"方法而言的，深度学习所学得的模型中，非线性操作的层级数更多。浅层学习依靠人工经验抽取样本特征，网络模型学习后获得的是没有层次结构的单层特征；而深度学习通过对原始信号进行逐层特征变换，将样本在原空间的特征表示变换到新的特征空间，自动地学习得到层次化的特征表示，从而更有利于分类或特征的可视化。深度学习理论的另外一个理论动机是：如果一个函数可用k层结构以简洁的形式表达，那么用k-1层的结构表达则可能需要指数级数量的参数(相对于输入信号)，且泛化能力不足。深度学习算法打破了传统神经网络对层数的限制，可根据设计者需要选择网络层数。它的训练方法与传统的神经网络相比有很大区别，传统神经网络随机设定参数初始值，采用BP算法利用梯度下降算法训练网络，直至收敛。但深度结构训练很困难，传统对浅层有效的方法对于深度结构并无太大作用，随机初始化权值极易使目标函数收敛到局部极小值，且由于层数较多，残差向前传播会丢失严重，导致梯度扩散，因此深度学习过程中采用贪婪无监督逐层训练方法。即在一个深度学习设计中，每层被分开对待并以一种贪婪方式进行训练，当前一层训练完后，新的一层将前一层的输出作为输入并编码以用于训练；最后每层参数训练完后，在整个网络中利用有监督学习进行参数微调。深度学习的概念最早由多伦多大学的G.E.Hinton等于2006年提出，基于样本数据通过一定的训练方法得到包含多个层级的深度网络结构的机器学习过程。传统的神经网络随机初始化网络中的权值，导致网络很容易收敛到局部最小值，为解决这一问题，Hinton提出使用无监督预训练方法优化网络权值的初值，再进行权值微调的方法，拉开了深度学习的序幕。深度学习所得到的深度网络结构包含大量的单一元素(神经元)，每个神经元与大量其他神经元相连接，神经元间的连接强度(权值)在学习过程中修改并决定网络的功能。通过深度学习得到的深度网络结构符合神经网络的特征，因此深度网络就是深层次的神经网络，即深度神经网络(deepneuralnetworks,DNN)。深度学习的概念起源于人工神经网络的研究，有多个隐层的多层感知器是深度学习模型的一个很好的范例。对神经网络而言，深度指的是网络学习得到的函数中非线性运算组合水平的数量。当前神经网络的学习算法多是针对较低水平的网络结构，将这种网络称为浅结构神经网络，如一个输入层、一个隐层和一个输出层的神经网络；与此相反，将非线性运算组合水平较高的网络称为深度结构神经网络，如一个输入层、三个隐层和一个输出层的神经网络。深度学习的基本思想：假设有系统S，它有n层（S1，…，Sn），输入为I，输出为O，可形象的表示为：I=&gt;S1=&gt;S2=&gt;…=&gt;Sn=&gt;O。为了使输出O尽可能的接近输入I，可以通过调整系统中的参数，这样就可以得到输入I的一系列层次特征S1，S2，…,Sn。对于堆叠的多个层，其中一层的输出作为其下一层的输入，以实现对输入数据的分级表达，这就是深度学习的基本思想。二、&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;国内外研究现状深度学习极大地促进了机器学习的发展，受到世界各国相关领域研究人员和高科技公司的重视，语音、图像和自然语言处理是深度学习算法应用最广泛的三个主要研究领域：1、深度学习在语音识别领域研究现状长期以来，语音识别系统大多是采用混合高斯模型(GMM)来描述每个建模单元的统计概率模型。由于这种模型估计简单，方便使用大规模数据对其训练，该模型有较好的区分度训练算法保证了该模型能够被很好的训练。在很长时间内占据了语音识别应用领域主导性地位。但是这种混合高斯模型实质上是一种浅层学习网络建模，特征的状态空间分布不能够被充分描述。而且，使用混合高斯模型建模方式数据的特征维数通常只有几十维，这使得特征之间的相关性不能被充分描述。最后混合高斯模型建模实质上是一种似然概率建模方式，即使一些模式分类之间的区分性能够通过区分度训练模拟得到，但是效果有限。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;从2009年开始，微软亚洲研究院的语音识别专家们和深度学习领军人物Hinton取得合作。2011年微软公司推出了基于深度神经网络的语音识别系统，这一成果将语音识别领域已有的技术框架完全改变。采用深度神经网络后，样本数据特征间相关性信息得以充分表示，将连续的特征信息结合构成高维特征，通过高维特征样本对深度神经网络模型进行训练。由于深度神经网络采用了模拟人脑神经架构，通过逐层地进行数据特征提取，最终得到适合进行模式分类处理的理想特征。深度神经网络建模技术，在实际线上应用时，能够很好地和传统语音识别技术结合，语音识别系统识别率大幅提升。国际上，谷歌也使用深层神经网络对声音进行建模，是最早在深度神经网络的工业化应用领域取得突破的企业之一。但谷歌的产品中使用的深度神经网络架构只有4、5层，与之相比百度使用的深度神经网络架构多达9层，正是这种结构上的差别使深度神经网络在线学习的计算难题得以更好的解决。这使得百度的线上产品能够采用更加复杂的神经网络模型。这种结构差异的核心其实是百度更好地解决了深度神经网络在线计算的技术难题，因此百度线上产品可以采用更复杂的网络模型。这对将来拓展大规模语料数据对深度神经网络模型的训练有更大的帮助。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2、深度学习在图像识别领域研究现状对于图像的处理是深度学习算法最早尝试应用的领域。早在1989年，加拿大多伦多大学教授YannLeCun就和他的同事们一起提出了卷积神经网络(ConvolutionalNeuralNetworks)。卷积神经网络也称为CNN，它是一种包含卷积层的深度神经网络模型。通常一个卷积神经网络架构包含两个可以通过训练产生的非线性卷积层，两个固定的子采样层和一个全连接层，隐藏层的数量一般至少在5个以上。CNN的架构设计是受到生物学家Hubel和Wiesel的动物视觉模型启发而发明的，尤其是模拟动物视觉皮层V1层和V2层中简单细胞(SimpleCell)和复杂细胞(ComplexCell)在视觉系统的功能。起初卷积神经网络在小规模的应用问题上取得了当时世界最好成果。但在很长一段时间里一直没有取得重大突破。主要原因是由于卷积神经网络应用在大尺寸图像上一直不能取得理想结果，比如对于像素数很大的自然图像内容的理解，这使得它没有引起计算机视觉研究领域足够的重视。直到2012年10月，Hinton教授以及他的两个学生采用更深的卷积神经网络模型在著名的ImageNet问题上取得了世界最好成果，使得对于图像识别的研究工作前进了一大步。Hinton构建的深度神经网络模型是使用原始的自然图像训练的，没有使用任何人工特征提取方法。自卷积神经网络提出以来，在图像识别问题上并没有取得质的提升和突破，直到2012年Hinton构建的深度神经网络才取得惊人成果。这主要是因为对算法的改进，在网络的训练中引入了权重衰减的概念，有效的减小权重幅度，防止网络过拟合。更关键的是计算机计算能力的提升，GPU加速技术的发展，这使得在训练过程中可以产生更多的训练数据，使网络能够更好的拟合训练样本。2012年国内互联网巨头百度公司将相关最新技术成功应用到人脸识别和自然图像识别问题，并推出了相应的产品。现在深度学习网络模型已能够理解和识别一般的自然图像。深度学习模型不仅大幅提高了图像识别的精度，同时也避免了需要消耗大量的时间进行人工特征提取的工作，使得在线运算效率大大提升。深度学习将有可能取代以往人工和机器学习相结合的方式成为主流图像识别技术。3、深度学习在自然语言处理领域研究现状自然语言处理(NLP)问题是深度学习在除了语音和图像处理之外的另一个重要应用领域。数十年以来，自然语言处理的主流方法是基于统计的模型，人工神经网络也是基于统计方法模型之一，但在自然语言处理领域却一直没有被重视。语言建模是最早采用神经网络进行自然语言处理的问题。美国的NEC研究院最早将深度学习引入到自然语言处理研究工作中，其研究人员从2008年起采用将词汇映射到一维矢量空间方法和多层一维卷积结构去解决词性标注、分词、命名实体识别和语义角色标注四个典型的自然语言处理问题。他们构建了同一个网络模型用于解决四个不同问题，都取得了相当精确的结果。总体而言，深度学习在自然语言处理问题上取得的成果和在图像语音识别方面还有相当的差距，仍有待深入探索。由于深度学习能够很好地解决一些复杂问题，近年来许多研究人员对其进行了深人研究，出现了许多有关深度学习研究的新进展。下面分别从初始化方法、网络层数和激活函数的选择、模型结构两个个方面对近几年深度学习研究的新进展进行介绍。1、&nbsp;初始化方法、网络层数和激活函数的选择研究人员试图搞清网络初始值的设定与学习结果之间的关系。Erhan等人在轨迹可视化研究中指出即使从相近的值开始训练深度结构神经网络，不同的初始值也会学习到不同的局部极值，同时发现用无监督预训练初始化模型的参数学习得到的极值与随机初始化学习得到的极值差异比较大，用无监督预训练初始化模型的参数学习得到的模型具有更好的泛化误差。Bengio与Krueger等人指出用特定的方法设定训练样例的初始分布和排列顺序可以产生更好的训练结果，用特定的方法初始化参数，使其与均匀采样得到的参数不同，会对梯度下降算法训练的结果产生很大的影响。Glorot等人指出通过设定一组初始权值使得每一层深度结构神经网络的Jacobian矩阵的奇异值接近1，在很大程度上减小了监督深度结构神经网络和有预训练过程设定初值的深度结构神经网络之间的学习结果差异。另外，用于深度学习的学习算法通常包含许多超参数，一些常用的超参数，尤其适用于基于反向传播的学习算法和基于梯度的优化算法。选择不同的网络隐层数和不同的非线性激活函数会对学习结果产生不同的影响。Glorot等人研究了隐层非线性映射关系的选择和网络的深度相互影响的问题，讨论了随机初始化的标准梯度下降算法用于深度结构神经网络学习得到不好的学习性能的原因。Glorot等人观察不同非线性激活函数对学习结果的影响，得到逻辑斯蒂S型激活单元的均值会驱使顶层和隐层进入饱和，因而逻辑斯蒂S型激活单元不适合用随机初始化梯度算法学习深度结构神经网络；并据此提出了标准梯度下降算法的一种新的初始化方案来得到更快的收敛速度。Bengio等人从理论上说明深度学习结构的表示能力随着神经网络深度的增加以指数的形式增加，但是这种增加的额外表示能力会引起相应局部极值数量的增加，使得在其中寻找最优值变得困难。2、&nbsp;模型结构(1)、DBN的结构及其变种：采用二值可见单元和隐单元RBM作为结构单元的DBN，在MNIST等数据集上表现出很好的性能。近几年，具有连续值单元的RBM，如mcRBM、mPoT模型和spike—and-slabRBM等已经成功应用。Spike—and—slabRBM中spike表示以0为中心的离散概率分布，slab表示在连续域上的稠密均匀分布，可以用吉布斯采样对spike—and—slabRBM进行有效推断，得到优越的学习性能。(2)、和--积网络；深度学习最主要的困难是配分函数的学习，如何选择深度结构神经网络的结构使得配分函数更容易计算?Poon等人提出一种新的深度模型结构----和--积网络(sum—productnetwork，SPN)，引入多层隐单元表示配分函数，使得配分函数更容易计算。SPN是有根节点的有向无环图，图中的叶节点为变量，中间节点执行和运算与积运算，连接节点的边带有权值，它们在Caltech-101和Olivetti两个数据集上进行实验证明了SPN的性能优于DBN和最近邻方法。(3)、基于rectified单元的学习：Glorot与Mesnil等人用降噪自编码模型来处理高维输入数据。与通常的S型和正切非线性隐单元相比，该自编码模型使用rectified单元，使隐单元产生更加稀疏的表示。对于高维稀疏数据，Dauphin等人采用抽样重构算法，训练过程只需要计算随机选择的很小的样本子集的重构和重构误差，在很大程度上提高了学习速度，实验结果显示提速了20倍。Glorot等人提出在深度结构神经网络中，在图像分类和情感分类问题中用rectified非线性神经元代替双曲正切或S型神经元，指出rectified神经元网络在零点产生与双曲正切神经元网络相当或者有更好的性能，能够产生有真正零点的稀疏表示，非常适合本质稀疏数据的建模，在理解训练纯粹深度监督神经网络的困难，搞清使用或不使用无监督预训练学习的神经网络造成的性能差异方面，可以看做新的里程碑；Glorot等人还提出用增加L1正则化项来促进模型稀疏性，使用无穷大的激活函数防止算法运行过程中可能引起的数值问题。在此之前，Nair等人提出在RBM环境中rectifed神经元产生的效果比逻辑斯蒂S型激活单元好，他们用无限数量的权值相同但是负偏差变大的一组单元替换二值单元，生成用于RBM的更好的一类隐单元，将RBM泛化，可以用噪声rectified线性单元(rectifiedlinearunits)有效近似这些S型单元。用这些单元组成的RBM在NORB数据集上进行目标识别以及在数据集上进行已标记人脸实际验证，得到比二值单元更好的性能，并且可以更好地解决大规模像素强度值变化很大的问题。(4)、卷积神经网络：研究了用生成式子抽样单元组成的卷积神经网络，在MNIST数字识别任务和Cahech一101目标分类基准任务上进行实验，显示出非常好的学习性能。Huang等人提出一种新的卷积学习模型----局部卷积RBM，利用对象类中的总体结构学习特征，不假定图像具有平稳特征，在实际人脸数据集上进行实验，得到性能很好的实验结果。三、&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;深度学习模型结构深度神经网络是由多个单层非线性网络叠加而成的，常见的单层网络按照编码解码情况分为3类：只包含编码器部分、只包含解码器部分、既有编码器部分也有解码器部分。编码器提供从输入到隐含特征空间的自底向上的映射，解码器以重建结果尽可能接近原始输入为目标将隐含特征映射到输入空间。人的视觉系统对信息的处理是分级的。从低级的提取边缘特征到形状（或者目标等），再到更高层的目标、目标的行为等，即底层特征组合成了高层特征，由低到高的特征表示越来越抽象。深度学习借鉴的这个过程就是建模的过程。深度神经网络可以分为3类，前馈深度网络(feed-forwarddeepnetworks,FFDN)，由多个编码器层叠加而成，如多层感知机(multi-layerperceptrons,MLP)、卷积神经网络(convolutionalneuralnetworks,CNN)等。反馈深度网络(feed-backdeepnetworks,FBDN)，由多个解码器层叠加而成，如反卷积网络(deconvolutionalnetworks,DN)、层次稀疏编码网络(hierarchicalsparsecoding,HSC)等。双向深度网络(bi-directionaldeepnetworks,BDDN)，通过叠加多个编码器层和解码器层构成(每层可能是单独的编码过程或解码过程，也可能既包含编码过程也包含解码过程)，如深度玻尔兹曼机(deepBoltzmannmachines,DBM)、深度信念网络(deepbeliefnetworks,DBN)、栈式自编码器(stackedauto-encoders,SAE)等。1、&nbsp;前溃深度网络前馈神经网络是最初的人工神经网络模型之一。在这种网络中，信息只沿一个方向流动，从输入单元通过一个或多个隐层到达输出单元，在网络中没有封闭环路。典型的前馈神经网络有多层感知机和卷积神经网络等。F.Rosenblatt提出的感知机是最简单的单层前向人工神经网络，但随后M.Minsky等证明单层感知机无法解决线性不可分问题(如异或操作)，这一结论将人工神经网络研究领域引入到一个低潮期，直到研究人员认识到多层感知机可解决线性不可分问题，以及反向传播算法与神经网络结合的研究，使得神经网络的研究重新开始成为热点。但是由于传统的反向传播算法，具有收敛速度慢、需要大量带标签的训练数据、容易陷入局部最优等缺点，多层感知机的效果并不是十分理想。1984年日本学者K.Fukushima等基于感受野概念，提出的神经认知机可看作卷积神经网络的一种特例。Y.Lecun等提出的卷积神经网络是神经认知机的推广形式。卷积神经网络是由多个单层卷积神经网络组成的可训练的多层网络结构。每个单层卷积神经网络包括卷积、非线性变换和下采样3个阶段，其中下采样阶段不是每层都必需的。每层的输入和输出为一组向量构成的特征图(featuremap)(第一层的原始输入信号可以看作一个具有高稀疏度的高维特征图)。例如，输入部分是一张彩色图像，每个特征图对应的则是一个包含输入图像彩色通道的二维数组(对于音频输入，特征图对应的是一维向量；对于视频或立体影像，对应的是三维数组)；对应的输出部分，每个特征图对应的是表示从输入图片所有位置上提取的特定特征。(1)、单层卷积神经网络：卷积阶段，通过提取信号的不同特征实现输入信号进行特定模式的观测。其观测模式也称为卷积核，其定义源于由D.H.Hubel等基于对猫视觉皮层细胞研究提出的局部感受野概念。每个卷积核检测输入特征图上所有位置上的特定特征，实现同一个输入特征图上的权值共享。为了提取输入特征图上不同的特征，使用不同的卷积核进行卷积操作。卷积阶段的输入是由n1个n2*n3大小的二维特征图构成的三维数组。每个特征图记为xi，该阶段的输出y，也是个三维数组，由m1个m2*m3大小的特征图构成。在卷积阶段，连接输入特征图xi和输出特征图yj的权值记为wij，即可训练的卷积核(局部感受野)，卷积核的大小为k2*k3，输出特征图为yj。非线性阶段，对卷积阶段得到的特征按照一定的原则进行筛选，筛选原则通常采用非线性变换的方式，以避免线性模型表达能力不够的问题。非线性阶段将卷积阶段提取的特征作为输入，进行非线性映射R=h(y)。传统卷积神经网络中非线性操作采用sigmoid、tanh或softsign等饱和非线性(saturatingnonlinearities)函数，近几年的卷积神经网络中多采用不饱和非线性(non-saturatingnonlinearity)函数ReLU(rectifiedlinearunits)。在训练梯度下降时，ReLU比传统的饱和非线性函数有更快的收敛速度，因此在训练整个网络时，训练速度也比传统的方法快很多。下采样阶段，对每个特征图进行独立操作，通常采用平均池化(averagepooling)或者最大池化(maxpooling)的操作。平均池化依据定义的邻域窗口计算特定范围内像素的均值PA，邻域窗口平移步长大于1(小于等于池化窗口的大小)；最大池化则将均值PA替换为最值PM输出到下个阶段。池化操作后，输出特征图的分辨率降低，但能较好地保持高分辨率特征图描述的特征。一些卷积神经网络完全去掉下采样阶段，通过在卷积阶段设置卷积核窗口滑动步长大于1达到降低分辨率的目的。(2)、卷积神经网络：将单层的卷积神经网络进行多次堆叠，前一层的输出作为后一层的输入，便构成卷积神经网络。其中每2个节点间的连线，代表输入节点经过卷积、非线性变换、下采样3个阶段变为输出节点，一般最后一层的输出特征图后接一个全连接层和分类器。为了减少数据的过拟合，最近的一些卷积神经网络，在全连接层引入"Dropout"或"DropConnect"的方法，即在训练过程中以一定概率P将隐含层节点的输出值(对于"DropConnect""指定数据"组的第一名；CASIAWS小组采用弱监督定位和卷积神经网络结合的方法，取得图形分类"额外数据"组的第一名，其分类错误率为11%。在目标定位任务中，VGG小组在深度学习框架Caffe的基础上，采用3个结构不同的卷积神经网络进行平均评估，以26%的定位错误率取得"指定数据"组的第一名；Adobe组选用额外的2000类ImageNet数据训练分类器，采用卷积神经网络架构进行分类和定位，以30%的错误率，取得了"额外数据"组的第一名。在多目标检测任务中，NUS小组采用改进的卷积神经网络----网中网(networkinnetwork,NIN)与多种其他方法融合的模型，以37%的平均准确率(meanaverageprecision,mAP)取得"提供数据"组的第一名；GoogLeNet以44%的平均准确率取得"额外数据"组的第一名。从深度学习首次应用于ILSVRC挑战赛并取得突出的成绩，到2014年挑战赛中几乎所有参赛队伍都采用深度学习方法，并将分类识错率降低到6.7%，可看出深度学习方法相比于传统的手工提取特征的方法在图像识别领域具有巨大优势。(2)、深度学习在人脸识别中的应用：基于卷积神经网络的学习方法，香港中文大学的DeepID项目以及Facebook的DeepFace项目在户外人脸识别(labeledfacesinthewild,LFW)数据库上的人脸识别正确率分别达97.45%和97.35%，只比人类识别97.5%的正确率略低一点点。DeepID项目采用4层卷积神经网络(不含输入层和输出层)结构，DeepFace采用5层卷积神经网络(不含输入层和输出层，其中后3层没有采用权值共享以获得不同的局部统计特征)结构，之后，采用基于卷积神经网络的学习方法。香港中文大学的DeepID2项目将识别率提高到了99.15%，超过目前所有领先的深度学习和非深度学习算法在LFW数据库上的识别率以及人类在该数据库的识别率。DeepID2项目采用和DeepID项目类似的深度结构，包含4个卷积层，其中第3层采用2*2邻域的局部权值共享，第4层没有采用权值共享，且输出层与第3、4层都全连接。(3)、深度学习在手写体字符识别中的应用：Bengio等人运用统计学习理论和大量的实验工作证明了深度学习算法非常具有潜力，说明数据中间层表示可以被来自不同分布而相关的任务和样例共享，产生更好的学习效果，并且在有62个类别的大规模手写体字符识别场景上进行实验，用多任务场景和扰动样例来得到分布外样例，并得到非常好的实验结果。Lee等人对RBM进行拓展，学习到的模型使其具有稀疏性，可用于有效地学习数字字符和自然图像特征。Hinton等人关于深度学习的研究说明了如何训练深度s型神经网络来产生对手写体数字文本有用的表示，用到的主要思想是贪婪逐层预训练RBM之后再进行微调。3、&nbsp;深度学习在行人检测中的应用将CNN应用到行人检测中，提出一种联合深度神经网络模型（unifieddeepnet，UDN）。输入层有3个通道，均为对YUV空间进行相关变换得到，实验结果表明在此实验平台前提下，此输入方式较灰色像素输入方式正确率提高8%。第一层卷积采用64个不同卷积核，初始化采用Gabor滤波器，第二层卷积采用不同尺度的卷积核，提取人体的不同部位的具体特征，训练过程作者采用联合训练方法。最终实验结果在Caltech及ETH数据集上错失率较传统的人体检测HOG-SVM算法均有明显下降，在Caltech库上较目前最好的算法错失率降低9%。4、&nbsp;深度学习在视频分类及行为识别中的应用A.Karpathy等基于卷积神经网络提供了一种应用于大规模视频分类上的经验评估模型，将Sports-1M数据集的100万段YouTube视频数据分为487类。该模型使用4种时空信息融合方法用于卷积神经网络的训练，融合方法包括单帧(singleframe)、不相邻两帧(latefusion)、相邻多帧(earlyfusion)以及多阶段相邻多帧(slowfusion)；此外提出了一种多分辨率的网络结构，大大提升了神经网络应用于大规模数据时的训练速度。该模型在Sports-1M上的分类准确率达63.9%，相比于基于人工特征的方法(55.3%)，有很大提升。此外，该模型表现出较好的泛化能力，单独使用slowfusion融合方法所得模型在UCF-101动作识别数据集上的识别率为65.4%，而该数据集的基准识别率为43.9%。S.Ji等提出一个三维卷积神经网络模型用于行为识别。该模型通过在空间和时序上运用三维卷积提取特征，从而获得多个相邻帧间的运动信息。该模型基于输入帧生成多个特征图通道，将所有通道的信息结合获得最后的特征表示。该三维卷积神经网络模型在TRECVID数据上优于其他方法，表明该方法对于真实环境数据有较好的效果；该模型在KTH数据上的表现，逊于其他方法，原因是为了简化计算而缩小了输入数据的分辨率。M.Baccouche等提出一种时序的深度学习模型，可在没有任何先验知识的前提下，学习分类人体行为。模型的第一步，是将卷积神经网络拓展到三维，自动学习时空特征。接下来使用RNN方法训练分类每个序列。该模型在KTH上的测试结果优于其他已知深度模型，KTH1和KTH2上的精度分别为94.39%和92.17%。七、&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;深度学习存在的问题及未来研究方向1、&nbsp;深度学习目前存在的问题：(1)、理论问题：深度学习在理论方面存在的困难主要有两个，第一个是关于统计学习，另一个和计算量相关。相对浅层学习模型来说，深度学习模型对非线性函数的表示能力更好。根据通用的神经网络逼近理论，对任何一个非线性函数来说，都可以由一个浅层模型和一个深度学习模型很好的表示，但相对浅层模型，深度学习模型需要较少的参数。关于深度学习训练的计算复杂度也是我们需要关心的问题，即我们需要多大参数规模和深度的神经网络模型去解决相应的问题，在对构建好的网络进行训练时，需要多少训练样本才能足以使网络满足拟合状态。另外，网络模型训练所需要消耗的计算资源很难预估，对网络的优化技术仍有待进步。由于深度学习模型的代价函数都是非凸的，这也造成理论研究方面的困难。(2)、建模问题：在解决深层学习理论和计算困难的同时，如何构建新的分层网络模型，既能够像传统深层模型一样能够有效的抽取数据的潜在特征，又能够像支持向量机一样便于进行理论分析，另外，如何针对不同的应用问题构建合适的深层模型同样是一个很有挑战性的问题。现在用于图像和语言的深度模型都拥有相似卷积和降采样的功能模块，研究人员在声学模型方面也在进行相应的探索，能不能找到一个统一的深度模型适用于图像，语音和自然语言的处理仍需要探索。(3)、工程应用问题：在深度学习的工程应用问题上，如何利用现有的大规模并行处理计算平台进行大规模样本数据训练是各个进行深度学习研发公司首要解决的难题。由于像Hadoop这样的传统大数据处理平台的延迟过高，不适用于深度学习的频繁迭代训练过程。现在最多采用的深度网络训练技术是随机梯度下降算法。这种算法不适于在多台计算机间并行运算，即使采用GPU加速技术对深度神经网络模型进行训练也是需要花费漫长的时间。随着互联网行业的高速发展，特别是数据挖掘的需要，往往面对的是海量需要处理的数据。由于深度学习网络训练速度缓慢无法满足互联网应用的需求。2、&nbsp;&nbsp;&nbsp;深度学习未来研究方向：深度学习算法在计算机视觉(图像识别、视频识别等)和语音识别中的应用，尤其是大规模数据集下的应用取得突破性的进展，但仍有以下问题值得进一步研究：(1)、无标记数据的特征学习目前，标记数据的特征学习仍然占据主导地位，而真实世界存在着海量的无标记数据，将这些无标记数据逐一添加人工标签，显然是不现实的。所以，随着数据集和存储技术的发展，必将越来越重视对无标记数据的特征学习，以及将无标记数据进行自动添加标签技术的研究。(2)、模型规模与训练速度训练精度之间的权衡。一般地，相同数据集下，模型规模越大，训练精度越高，训练速度会越慢。例如一些模型方法采用ReLU非线性变换、GPU运算，在保证精度的前提下，往往需要训练5~7d。虽然离线训练并不影响训练之后模型的应用，但是对于模型优化，诸如模型规模调整、超参数设置、训练时调试等问题，训练时间会严重影响其效率。故而，如何在保证一定的训练精度的前提下，提高训练速度，依然是深度学习方向研究的课题之一。(3)、理论分析需要更好地理解深度学习及其模型，进行更加深入的理论研究。深度学习模型的训练为什么那么困难?这仍然是一个开放性问题。一个可能的答案是深度结构神经网络有许多层，每一层由多个非线性神经元组成，使得整个深度结构神经网络的非线性程度更强，减弱了基于梯度的寻优方法的有效性；另一个可能的答案是局部极值的数量和结构随着深度结构神经网络深度的增加而发生定性改变，使得训练模型变得更加困难。造成深度学习训练困难的原因究竟是由于用于深度学习模型的监督训练准则大量存在不好的局部极值，还是因为训练准则对优化算法来说过于复杂，这是值得探讨的问题。此外，对堆栈自编码网络学习中的模型是否有合适的概率解释，能否得到深度学习模型中似然函数梯度的小方差和低偏差估计，能否同时训练所有的深度结构神经网络层，除了重构误差外，是否还存在其他更合适的可供选择的误差指标来控制深度结构神经网络的训练过程，是否存在容易求解的RBM配分函数的近似函数，这些问题还有待未来研究。考虑引入退火重要性抽样来解决局部极值问题，不依赖于配分函数的学习算法也值得尝试。(4)、数据表示与模型数据的表示方式对学习性能具有很大的影响，除了局部表示、分布表示和稀疏分布表示外，可以充分利用表示理论研究成果。是否还存在其他形式的数据表示方式，是否可以通过在学习的表示上施加一些形式的稀疏罚从而对RBM和自编码模型的训练性能起到改进作用，以及如何改进。是否可以用便于提取好的表示并且包含更简单优化问题的凸模型代替RBM和自编码模型；不增加隐单元的数量，用非参数形式的能量函数能否提高RBM的容量等，未来还需要进一步探讨这些问题。此外，除了卷积神经网络、DBN和堆栈自编码网络之外，是否还存在其他可以用于有效训练的深度学习模型，有没有可能改变所用的概率模型使训练变得更容易，是否存在其他有效的或者理论上有效的方法学习深度学习模型，这也是未来需要进一步研究的问题。现有的方法，如DBN．HMM和DBN—CRF，在利用DBN的能力方面只是简单的堆栈叠加基本模型，还没有充分发掘出DBN的优势，需要研究DBN的结构特点，充分利用DBN的潜在优势，找到更好的方法建立数据的深度学习模型，可以考虑将现有的社会网络、基因调控网络、结构化建模理论以及稀疏化建模等理论运用其中。(5)、特征提取除了高斯--伯努利模型之外，还有哪些模型能用来从特征中提取重要的判别信息，未来需要提出有效的理论指导在每层搜索更加合适的特征提取模型。自编码模型保持了输入的信息，这些信息在后续的训练过程中可能会起到重要作用，未来需要研究用CD训练的RBM是否保持了输入的信息，在没有保持输入信息的情况下如何进行修正。树和图等结构的数据由于大小和结构可变而不容易用向量表示其中包含的信息，如何泛化深度学习模型来表示这些信息，也是未来需要研究的问题。尽管当前的产生式预训练加判别式微调学习策略看起来对许多任务都运行良好，但是在某些语言识别等其他任务中却失败了，对这些任务，产生式预训练阶段的特征提取似乎能很好地描述语音变化，但是包含的信息不足以区分不同的语言，未来需要提出新的学习策略，对这些学习任务提取合适的特征，这可以在很大程度上减小当前深度学习系统所需模型的大小。(6)、训练与优化求解为什么随机初始化的深度结构神经网络采用基于梯度的算法训练总是不能成功，产生式预训练方法为什么有效?未来需要研究训练深度结构神经网络的贪婪逐层预训练算法到底在最小化训练数据的似然函数方面结果如何，是否过于贪婪，以及除了贪婪逐层预训练的许多变形和半监督嵌入算法之外，还有什么其他形式的算法能得到深度结构神经网络的局部训练信息。此外，无监督逐层训练过程对训练深度学习模型起到帮助作用，但有实验表明训练仍会陷入局部极值并且无法有效利用数据集中的所有信息，能否提出用于深度学习的更有效的优化策略来突破这种限制，基于连续优化的策略能否用于有效改进深度学习的训练过程，这些问题还需要继续研究。二阶梯度方法和自然梯度方法在理论研究中可证明对训练求解深度学习模型有效，但是这些算法还不是深度结构神经网络优化的标准算法，未来还需要进一步验证和改进这些算法，研究其能否代替微批次随机梯度下降类算法。当前的基于微批次随机梯度优化算法难以在计算机上并行处理，目前最好的解决方法是用GPU来加速学习过程，但是单个机器的GPU无法用于处理大规模语音识别和类似的大型数据集的学习，因此未来需要提出理论上可行的并行学习算法来训练深度学习模型。(7)、与其他方法的融合从上述应用实例中可发现，单一的深度学习方法，往往并不能带来最好的效果，通常融合其他方法或多种方法进行平均打分，会带来更高的精确率。因此，深度学习方法与其他方法的融合，具有一定的研究意义。(8)、研究拓展当深度模型没有有效的自适应技术，在测试数据集分布不同于训练集分布时，它们很难得到比常用模型更好的性能，因此未来有必要提出用于深度学习模型的自适应技术以及对高维数据具有更强鲁棒性的更先进的算法。目前的深度学习模型训练算法包含许多阶段，而在在线学习场景中一旦进入微调阶段就有可能陷入局部极值，因此目前的算法对于在线学习环境是不可行的。未来需要研究是否存在训练深度学习的完全在线学习过程能够一直具有无监督学习成分。DBN模型很适合半监督学习场景和自教学习场景，当前的深度学习算法如何应用于这些场景并且在性能上优于现有的半监督学习算法，如何结合监督和无监督准则来学习输入的模型表示，是否存在一个深度使得深度学习模型的计算足够接近人类在人工智能任务中表现出的水平，这也是未来需要进一步研究的问题。八、&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;深度学习开源软件目前网上关于深度学习的源代码非常多，http://meta-guide.com/software-meta-guide/100-best-github-deep-learning列出了最常用的100个关于深度学习的开源项目。最近google也开源了TensorFlow。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;以上整理的内容主要摘自五篇文章：1、&nbsp;&nbsp;&nbsp;《基于深度学习的图像检索研究》，内蒙古大学，硕论，20142、&nbsp;&nbsp;&nbsp;《基于卷积神经网络的深度学习算法与应用研究》，浙江工商大学，硕论，20143、&nbsp;&nbsp;&nbsp;《深度学习研究进展》，计算机应用研究，期刊，20144、&nbsp;&nbsp;&nbsp;《深度学习研究综述》，北京工业大学
关键词: 深度学习, 语音识别, 图像识别自然语言处理
AI技术: 深度学习，机器学习，自然语言处理，卷积神经网络，语音识别
行业: 语音识别, 图像识别自然语言处理
重大事件摘要: 这篇文章详细介绍了深度学习的概念、研究现状以及在各个领域的应用情况。以下是文章中的重大事件总结：

1. **深度学习的提出与发展**: 深度学习由Hinton等人于2006年提出，标志着机器学习进入了一个新的发展阶段。这一概念通过引入多层神经网络模型和无监督学习技术，极大地推动了语音识别、图像识别和自然语言处理等领域的发展。

2. **深度学习模型结构与训练算法**: 文章介绍了深度学习模型的基本结构和训练算法，强调了深度神经网络在特征提取方面的能力，以及如何通过大规模数据训练来提高分类和预测的精度。特别提到了卷积神经网络（CNN）在图像识别领域的应用，以及其在ImageNet竞赛中取得的突破性成果。

3. **深度学习在不同领域的应用进展**:
   - **语音识别**: 深度学习的应用显著提高了语音识别的准确性，尤其是在采用深度神经网络后，错误率大幅降低。
   - **图像识别**: 卷积神经网络（CNN）的发展使得图像识别技术取得了巨大进步，特别是在大型数据集上的训练和应用。
   - **自然语言处理**: 深度学习在自然语言处理领域的应用也在逐步深入，尽管与图像和语音识别相比还有一定差距，但在词性标注、分词等方面已取得显著成果。

4. **国内外研究现状与未来方向**: 文章还概述了国内外在深度学习领域的研究现状，包括美国国防部对深度学习项目的支持，以及国内企业和研究机构在这一领域的活跃探索和实践。同时，指出了深度学习面临的挑战，如计算资源的需求、模型复杂度的管理等，并展望了未来的研究方向。

5. **开源软件与社区贡献**: 文章提到了深度学习领域的开源软件，如TensorFlow和Theano，这些工具为研究人员提供了强大的支持，促进了深度学习技术的普及和发展。

这篇文章全面地概述了深度学习的历史背景、理论基础、技术发展、应用案例以及面临的挑战和未来趋势，为读者提供了一个关于深度学习领域的宏观视角。
