标题: MIT学者独家撰文:ChatGPT的瓶颈与解药
链接: https://baijiahao.baidu.com/s?id=1782716571221570880&wfr=spider&for=pc
总结: 图片来源@视觉中国文 | 甲子光年科技产业智库，作者｜罗鸿胤，编辑｜王博、苏霍伊*本文为麻省理工学院（MIT）学者罗鸿胤独家供稿，「甲子光年」经其授权后编辑发布。罗鸿胤是人工智能领域的青年科学家、MIT 计算机学与人工智能实验室（CSAIL）的博士后研究员，主要关注自然语言处理方向，包括自训练算法、蕴含模型、语言模型推理问题。他博士毕业于 MIT 电子工程与计算机科学系，师从 Jim Glass 博士；本科毕业于清华大学计算机系，师从刘知远教授。人工智能领域一直存在着学派之争。曾经，“建制派”的符号主义 AI 被看作“唯一的主导力量”，“逻辑驱动”的人工智能曾主宰数十年；另一派则是代表经验主义 AI 的深度学习，不追求解释和逻辑，以神经网络和大数据开启”暴力美学“的大门。以 GPT 系列为代表的大语言模型就是这条“暴力美学”路线的产物。这条路现在看来是成功的，但也存在一定的局限性。从人工智能诞生的第一天起，计算机科学家们一直在比较以神经网络为代表的经验主义 AI 与以数理逻辑为代表的符号主义 AI 的优劣。简单来说，经验主义 AI 主张通过对大量数据的学习来获取知识，而符号主义 AI 则强调精确的任务定义和严谨的数学工具。随着近十年的算力进化，神经网络这一最典型的经验主义 AI 模型得到了飞速的发展。由于无法匹敌神经网络处理非结构化信息的能力和泛用性、无法生成非结构化数据（如自然语言），符号主义 AI 的存在感和影响力快速降低。但是在我看来，基于符号和逻辑的推理 (reasoning) 远比基于经验和数据的感知 (perception) 复杂。经验主义 AI 发展的顶点，正是符号主义 AI 大放异彩的起点。著名语言模型批评者 Gary Marcus 博士曾锐评道：“大语言模型没法做一些有严格定义的工作：遵守国际象棋规则、五位数字相乘、在家谱中进行可靠的推理、比较不同物体的重量等等。”“火力全开”的 Marcus 博士指出了目前大语言模型存在的问题，但是这个问题并非没有解决方法，我认为：大语言模型（LLM）只是不能通过生成文本做有严格定义的工作。大语言模型可以通过生成 “自然语言嵌入式程序” （natural language embedded program, NLEP）准确完成上述工作。NLEP 是我与麻省理工学院（MIT）、香港中文大学（CUHK）研究团队共同研发的一种兼顾符号推理和自然语言生成的程序。它将语言智能抽象为「“思维”编程 + 程序执行」两个步骤，能让大语言模型同时具有生成自然语言和精确执行复杂推理任务的能力。在传统认知里，符号 AI 无法处理非结构化数据和生成自然语言。而 NLEP 的方法证明，符号 AI 可以处理非结构化数据、自然语言，还可以强化非结构化数据深层的结构规律和推理能力。或许在不久的将来，符号主义有潜力替代经验主义。接下来，我将从 Marcus 博士的锐评出发，讨论以下内容：当前最先进的神经网络模型其实与醉酒的人相似。他们都努力与人互动、跟随简单指令生成信息，少数还试图驾驶交通工具。同时，他们也都带来了商业机遇和社会风险，并可能引起广泛讨论。人类认知功能不完整时（如醉酒、梦呓、疾病等），语言行为往往是脱离逻辑思维的。这时，人类只是依赖语言本能，把输入信号强行拼凑成有一定语法结构的句子（文本补全）。表达的内容可能是如李白斗酒诗百篇般的艺术瑰宝，也可能只是毫无意义的胡言乱语。事实上，人类大脑语言区域的发现正是基于临床医生对认知功能受损、保留了部分语言能力患者的研究。类似的科学方法也被大量应用于探索 AI 模型行为和规律的研究中。随着算力的快速发展，OpenAI 等机构花费数百亿美元构建了参数量远超人类语言器官的神经网络，和文本量远超人类阅读极限的训练数据，为体积远大于人脑的机器赋予了类似的文本补全能力。但此类模型生成的究竟是 “语言” 还是 “梦呓”？这个问题已经在学术界引起了激烈争论。争论的结果关乎社会和业界对 AI 可解释性、可靠性、安全性的认可程度。而决定结果的关键就在于语言模型是否存在可控、准确的思维能力。为了回答这一核心问题，谷歌旗下研究机构 DeepMind 的最新论文指出，语言模型本质上是信息的压缩模型。只要模型的表示能力足够强（参数量足够）、被压缩的训练数据量足够大，语言模型就能在压缩信息的过程中抽象出一定的思维能力，包括推理、计算、预测等等。最先进的语言模型（例如 GPT-4）展现出的回答问题、跟随指令、编写代码的能力显然早已超越了任何人类的 “梦呓”。但如果说 GPT-4 和基于 GPT-4 的种种 Agent 足够可靠，似乎为时尚早。GPT-4 是极端经验主义 AI 的代表：把世界上所有的高质量文本、程序、数学、对话数据压缩到算力允许的最大模型里，再抽象出这一技术路线蕴含的最强思维能力。它没有可靠推理引擎的支撑，完全依赖简单粗暴、类似“死记硬背”的大量训练。无论多少计算和数据资源，都无法掩盖和弥补 GPT-4 本质的推理缺陷。就如同酒驾的司机，无论酒量多好、多么侥幸，都无法避免酒精对人反应和判断能力的本质危害。正如不同的任务对人的思维严谨程度有不同要求，当前的语言模型更适用于能容忍甚至欢迎一些噪声的应用场景，但在需要执行准确、可控的复杂推理任务时，其可靠性有根本的缺陷。GPT-4 甚至会在回答一些并不复杂的问题时生成自相矛盾的文本，如下图所示：实际上，吴丹（U Thant）是第一位来自于亚洲的联合国秘书长，潘基文（Ban Ki-moon）是第二位来自于亚洲的联合国秘书长，上图中 GPT-4 的回答并不准确。能力如此强大的 GPT-4，却依然会在简单的问答中生成自相矛盾的语言，这也佐证了现阶段语言模型推理的不可靠性。人类运用语言的能力可以抽象成知识、推理、计算三大模块，并且语言绝对不等于文本。许多语言模型（文本补全模型）的问题难以解决，绝非模型不够强大，而是因为自然语言文本是思维结果的表达，并不是思维过程的载体。比如，我们想要学好物理，“事半功倍”的办法就需要从物理定律、求解问题、设计实验的思路出发；反之“事倍功半”的办法则是死记硬背一百本物理习题却不理解牛顿定律。采用这种方法的学习者花费更多的时间，但还是无法融会贯通地解决没见过的问题。这个缺陷并不是解题模型——人类大脑的问题，而是训练数据的缺陷——问题的答案只是物理定律的表象，而解题思维代表着对物理定律的直接应用。不可否认，“死记硬背”是实现“答对考题”的技术路线之一。与之相似，使用大型神经网络在大规模数据集上学习文本补全能力，也是当前 AI “获得思维”的技术路线。虽然巨量的计算资源与数据的投入让这种技术路线取得了成功，但诸多的研究和应用已经证明，这种技术路线的可靠性瓶颈会带来诸多挑战：臆想、推理能力有限、隐私泄露、合规问题等等。大语言模型的能力是一把双刃剑：可以处理不存在于训练数据中的新问题，但也会在其不知情的情况下，输出错误的推理结果。作为通过压缩文本提炼思维的黑盒模型，其知识、思维、推理能力都储存在神经网络的权重中。AI 的优势和不足都体现在以下几个方面：由于以上三个模块都有可能出错，大模型的行为难以验证、解释、控制、改进。针对“在美国，哪种新冠病毒造成了最高的 ICU 占用量”这个问题，GPT-4模型的回答是“德尔塔变种导致的 ICU 占用量最高”。那真实的情况是什么？在 11 月 6 日的 OpenAI 开发日前，没有搜索引擎增强的 GPT-4 模型会给出定性的回答和解释：开发日后的 GPT-4 系统默认调用必应搜索引擎，会基于搜索结果给出数据、作出一定解释和参考资料引用：中文翻译：获得搜索增强的 ChatGPT 生成了更有说服力、文本更专业的回复。尤其是在其中三处引用了参考资料网址，更加提高了用户阅读答案后的满意度（和被误导的可能性）。遗憾的是， ChatGPT 的用户很难验证答案的正确性。事实上，重复问最新的（2023 年 11 月 13 日）、搜索引擎加持的 GPT-4 同样的问题，它还会生成各种不同的回答:回答 a：“奥密克戎变异 – 占用了高达 30.4% 的 ICU 病床。”回答 b：“虽然感染了德尔塔变异的病人最多占用了 31% 的 ICU 病床，但奥密克戎病人占用了更多。”回答 c：“好像不是奥密克戎变异，好像是德尔塔变异。”虽然在不同尝试中 GPT-4 的回答自相矛盾，但是每一次回答生成的文本看起来都很正式、客观、有说服力、甚至附带搜索引擎给出的参考文献。未经多次验证答案的读者很容易受到误导。语言模型的这种能力非常适合于创作和想象：给一个标题，写三个小故事之类的任务对于 ChatGPT 而言恰到好处。但遗憾的是，这种不可控的行为模式，在回答需要严谨推理的问题时应该被尽量避免。更遗憾的是，虽然给了 GPT-4 多次尝试的机会甚至搜索引擎的加持，上述新老 GPT-4 猜测的答案中没有一个是正确的。根据权威统计机构数据看世界（Our World in Data）信息，美国因新冠病毒导致的 ICU 病床日占用量峰值应发生在 2020 年冬天阿尔法变异流行期间。GPT-4 基于必应搜索引擎提供的大量“比较德尔塔与奥密克戎变种病毒”的文章得出“德尔塔或奥密克戎变异造成了最高的 ICU 病床占用量”是不准确的。那么，GPT-4 在知识、推理、计算的哪一步出现了错误？是搜索的数据出了问题，还是对于三个峰值比较大小的运算出了问题？用户并不了解。在上述例子中，GPT-4 的可解释性和可靠性都会受到质疑。为了改进语言模型的事实性、可解释性、可控性和可靠性，OpenAI、Meta、麻省理工学院、香港中文大学（CUHK）、卡耐基梅隆大学、滑铁卢大学等机构的研究人员分别提出了不同的基于编程语言以及程序解释器增强的技术方案。其中，比较广为人知的方案是 OpenAI 开发的 ChatGPT 代码解释器和 Meta 提出的 Toolformer 模型。它们在文本生成的过程中将一部分内容“外包”给程序或 API，例如数学运算。代码解释器或者可靠 API 能够保证在输入正确的情况下永远计算出一致、正确的结果，并将结果返回到语言模型生成的内容里，比如：最后的总分是由一段 python 代码计算得到：虽然“外包”了一部分推理任务给可靠的代码解释器，ChatGPT 的主干仍然是自然语言。上述例子只在最后一步计算总分时调用了代码解释器，而步骤 3 中 “30 分” 的中间结果仍然是由自然语言完成的推理。最新的研究表明，在很多任务上 ChatGPT 负责调用代码解释器的数据分析(Data Analysis) Agent 仍不能取得准确的推理效果。比如，它拒绝用代码解决一些非结构化问题中的结构化推理任务，因此得到错误的结果：在这个例子中，我们的问题是“有几位联合国秘书长不是来自欧洲？”虽然使用了 ChatGPT 的数据分析 agent，但它拒绝使用代码分析，而是使用自然语言“敷衍了事”。这也就造成了，虽然 GPT-4 生成了正确的人物列表及国籍，最后的计数却漏了来自亚洲的潘基文秘书长。这里正确答案应为 5 位联合国秘书长来自欧洲，而 ChatGPT 数据分析 Agent 偷工减料推理得到的结果是 4 位。NLEP 是一种同时提高自然语言、符号推理能力的神经符号 (neuro-symbolic) 方法。针对 ChatGPT 代码解释器的种种痛点，麻省理工学院（MIT）和香港中文大学（CUHK）的研究人员提出了一个大胆的假设：“哪里有自然语言，哪里就有不严谨的思维。”基于这种假设，我们提出了一种独特的语言生成方案：natural language embedded program (NLEP，自然语言嵌入式程序)。OpenAI 采取了“文本补全+代码解释器插件”的范式，在自然语言中必要处添加代码和插件的调用。NLEP 则通过生成可一键运行的程序解决一切自然语言、数学、符号推理、编程问题，只在程序中必要的地方嵌入自然语言。在完成程序生成后，点击“运行”按钮，由程序打印出自然语言的回答。例如在之前的联合国秘书长计数问题中，NLEP 生成的内容如下：在图中可以看到，语言模型生成了一段逐步解决问题的程序：定义结构化知识、实现计算结果的函数、打印自然语言回复。完成程序的生成后，运行完整的程序，即可得到正确的结果。在五次独立重复实验中， GPT-4 API 的正确率为 40%，ChatGPT 代码解释器的正确率为 60%，而 NLEP 的正确率为 100%。NLEP 与 ChatGPT 代码解释器相比有显著的区别：ChatGPT 以自然语言文本为主干回复用户输入。在生成某个词的时候切换到代码运行，再将代码运行结果添加到生成的内容里，然后继续生成文本；而 NLEP 以程序为主干，首先生成完整的程序，然后执行程序、打印出包含自然语言文本、图表等要素的回复。同时，NLEP 的编程语言框架也可以比自然语言框架更自然地链接数据。相比于自然语言框架，NLEP 作为完整的可运行程序，可以更自然地链接知识库和数据库。NLEP 可以准确调用谷歌知识图谱里的真实数据，回答此前“哪个新冠变种导致了最高的 ICU 日占用率”的问题并提供数据可视化作为解释：NLEP 的回答是“The COVID variant caused the highest daily ICU occupation in United States is Alpha (在美国造成最高 ICU 占用的新冠病毒变种是阿尔法).”并以此生成出自动可视化数据：以上功能由 NLEP 的生成工具 LangCode 实现。此外，NLEP 还可以自动生成结构化 Agent。NLEP 与 ChatGPT 的本质区别在于是否采用结构化的语言生成框架。ChatGPT 以非结构化的自然语言文本补全为基本范式。因此在上周的 OpenAI 开发日，OpenAI 公布的 GPT store 也更多集中于非结构化的 agent，即 chatbot 的自动搭建。而早在 OpenAI 公布 GPT store 一个月前，我们就利用融合了符号、结构、自然语言的能力的 NLEP 为 Anchoring AI 平台实现了自动生成结构化 Agent 的功能。如图所示，Anchoring AI Agent 可以服务结构化的输入和输出。其推理过程、自动生成的提示信息也显示在自动生成的独立模块中，透明可控、清晰准确，便于团队协作开发AI应用。如 GPTs Agent:以及根据一句自然语言指令自动生成的Anchoring.ai Agent:经验主义与符号主义AI争议纷扰六十余年，其核心矛盾在于：经验主义 AI 侧重强大的泛化能力，而符号主义AI侧重精确地推理能力。近二十年来，拔地而起、粗放增长的 AI 研究和产业强调扩展 AI 的应用场景。因此，泛化能力成为了近十年 AI 的主题。尤其在 ChatGPT 横空出世的 2022 年底，经验主义 AI 发展到了极致：GPT 模型有着极强的泛化性能，能够处理非常广泛的数据和应用。但在后 GPT-4 时代，AI 的粗放增长会迅速来到瓶颈期，转而进入精益发展的阶段。下一个十年AI领域的主题将是精确推理、可解释性、安全可控。依托于经验主义AI的坚实基础和强大泛化能力，符号主义将接过解决AI诸多挑战的重任，在未来的AI发展中大放异彩，带来无数崭新的可能。甲小姐对本文亦有贡献*本文配图由作者提供举报/反馈
关键词: ChatGPT, 瓶颈, 解药
AI技术: 大语言模型, GPT-4, 自然语言嵌入式程序NLEP
行业: 麻省理工学院,  香港中文大学,  卡耐基梅隆大学
重大事件摘要: 这篇文章是一篇关于人工智能领域，特别是大型语言模型（LLMs）和符号主义AI的深度分析。文章由MIT学者罗鸿胤撰写，探讨了ChatGPT等大模型的瓶颈问题，并提出了一种新的解决方案——自然语言嵌入式程序（NLEP）。以下是文章中的重大事件总结：

1. **背景介绍**：文章首先介绍了人工智能领域的学派之争，特别是经验主义AI（如深度学习）和符号主义AI（如逻辑推理）之间的对比。随着算力的提升，深度学习模型，尤其是大型语言模型（LLMs），在处理非结构化信息方面取得了显著进展。

2. **问题提出**：文章指出，尽管大型语言模型（LLMs）在生成文本、回答问题等方面表现出色，但在执行需要严格定义的任务时存在局限性。例如，它们不能准确执行国际象棋规则、乘法或比较物体重量等任务。

3. **解决方案介绍**：为了解决这些问题，文章介绍了一种名为“自然语言嵌入式程序”（NLEP）的新方法。NLEP是一种结合符号推理和自然语言生成的程序，它允许大型语言模型通过生成可执行的程序来准确完成复杂任务。

4. **实验结果**：文章展示了NLEP在多个任务上的性能，包括联合国秘书长计数、新冠病毒变种导致的ICU占用量峰值等。实验结果表明，NLEP在这些任务上的表现优于传统的大型语言模型。

5. **技术细节**：文章详细描述了NLEP的工作原理，包括如何将自然语言指令转换为可执行的程序，以及如何利用这些程序来提高大型语言模型的准确性和可靠性。

6. **未来展望**：最后，文章讨论了NLEP对未来人工智能发展的影响，特别是在精确推理、可解释性和安全可控性方面的潜在应用。作者认为，随着经验的积累和技术的发展，符号主义AI有可能在未来十年内接过解决AI挑战的重任。
