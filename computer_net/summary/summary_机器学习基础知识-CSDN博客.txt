标题: 机器学习基础知识-CSDN博客
链接: http://www.baidu.com/link?url=4bEIgcOo2NDeQKdtNJ8nctejwwYI73ISGitfbUthVir2_2BjRMGKpzlwUHMIDraBPLCfJw9XDuIDvXDmBxrN0Gd7PFMQFT8DgrrvfSHrIwa
总结: 机器学习(MachineLearning)是让计算机能够自动地从某些数据中总结出规律，并得出某种预测模型，进而利用该模型对未知数据进行预测的方法。它是一种实现人工智能的方式，是一门交叉学科，综合了统计学、概率论、逼近论、凸分析、计算复杂性理论等。机器学习分类目前，机器学习大致可以分为以下几类：(1)有监督学习(SupervisedLearning)：当我们已经拥有–些数据及数据对应的类标时，就可以通过这些数据训练出一个模型，再利用这个模型去预测新数据的类标，这种情况称为有监督学习。有监督学习可分为回归问题和分类问题两大类。在回归问题中，我们预测的结果是连续值;而在分类问题中，我们预测的结果是离散值。常见的有监督学习算法包括线性回归、逻辑回归、K-近邻、朴素贝叶斯、决策树、随机森林、支持向量机等。(2)无监督学习(UnsupervisedLearning)：在无监督学习中是没有给定类标训练样本的，这就需要我们对给定的数据直接建模。常见的无监督学习算法包括K-means、EM算法等。(3)半监督学习(Semi-supervisedLearn-ing)：半监督学习介于有监督学习和无监督学习之间，给定的数据集既包括有类标的数据，也包括没有类标的数据，需要在工作量(例如数据的打标)和模型的准确率之间取一个平衡点。(4)强化学习(ReinforcementLearning)：从不懂到通过不断学习、总结规律，最终学会的过程便是强化学习。强化学习很依赖于学习的“周围环境”，强调如何基于“周围环境”而做出相应的动作。机器学习的一般流程一个机器学习任务的成功与否往往在很大程度上取决于特征工程。简单来说，特征工程的任务是从原始数据中抽出最具代表性的特征，从而让模型能够更有效地学习这些数据。通常我们可以使用scikit-learn这个库来处理数据和提取特征，scikit-learn是机器学习中使用非常广泛的第三方模块，本身封装了很多常用的机器学习算法，同时还有很多数据处理和特征提取相关的方法。数据预处理根据数据类型的不同，数据预处理的方式和内容也不尽相同，这里简单介绍几种较常用的方式。**(1)归一化归一化指将不同变化范围内的值映射到一个固定的范围里，例如，常使用min-max等方法将数值归一化到[0，1]的区间内(有些时候也会归一化到[-1，1]的区间内)。归一化的作用包括无量纲化一、加快模型的收敛速度，以及避免小数值的特征被忽略等。(2)标准化标准化指在不改变数据原分布的前提下，将数据按比例缩放，使之落入一个限定的区间，让数据之间具有可比性。需要注意的是，归一化和标准化各有其适用的情况，例如在涉及距离度量或者数据符合正态分布的时候，应该使用标准化而不是归一化。常用的标准化方法有z-score等。(3)离散化离散化指把连续的数值型数据进行分段，可采用相等步长或相等频率等方法对落在每一个分段内的数值型数据赋予一个新的统一的符号或数值。离散化是为了适应模型的需要，有助于消除异常数据，提高算法的效率。(4)二值化二值化指将数值型数据转换为0和1两个值，例如通过设定一个阈值，当特征的值大于该阈值时转换为1，当特征的值小于或等于该阈值时转换为0。二值化的目的在于简化数据，有些时候还可以消除数据(例如图像数据)中的“杂音”。特征工程特征工程的目的是把原始的数据转换为模型可用的数据，主要包括三个子问题：特征构造、特征提取和特征选择。特征构造一般是在原有特征的基础上做“组合”操作，例如，对原有特征进行四则运算，从而得到新的特征。特征提取指使用映射或变换的方法将维数较高的原始特征转换为维数较低的新的特征。如主成分分析特征选择即从原始的特征中挑选出一些具有代表性、使模型效果更好的特征。其中，特征提取和特征选择最为常用。模型性能判别与选择基础概念在分类任务中，通常把错分的样本数占样本总数的比例称为错误率（errorrate）。比如m个样本有a个预测错了，错误率就是E=a/m；与错误率相对的1-a/m称为精度（accuracy），或者说正确率，数值上精度=1-错误率。更一般地，我们通常会把学习器的实际预测输出与样本的真实输出之间的差异称为误差（error）。学习器在训练集上的误差称为训练误差（trainingerror）或者经验误差（empiricalerror）。而在新样本上的误差则称为泛化误差（generalizationerror）或者测试误差(testerror;)。显然，我们希望得到泛化误差小的学习器。所以我们希望模型的泛化误差尽可能小，但现实是，我们无法知道新样本是怎样的，所以只能尽可能地利用训练数据来最小化经验误差。Llossfunction损失函数过(欠)拟合对于经验误差很小，而在新样本中泛化误差很大的情况，多是由于在测试集中将训练样本中的一些本身特征看作为整个样本空间的特征，从而导致泛化能力下降。而欠拟合则是相反。有多种因素可能导致过拟合，其中最常见的情况是由于学习能力过于强大，以至于把训练样本所包含的不太一般的特性都学到了，而欠拟合则通常是由于学习能力低下而造成的。欠拟合比较容易克服，只要适当地增加模型复杂度（比方说增加神经网络的层数或者训练轮数，扩展决策树学习中的分支）就好。但过拟合是无法彻底避免的，我们所能做的只是缓解，或者说减小其风险（比如减少模型复杂度/增加训练数据），这也是机器学习发展中的一个关键阻碍。这样，在学习时就要防止过拟合，进行最优的模型选择，即选择复杂度相当的模型，以达到使测试误差最小的学习目的。下面介绍几种常用的模型选择方法。留出法，正则化，交叉验证，自助法模型性能评价错误率与精度查准率，查全率，F1，P-R曲线ROC曲线在不同的应用任务中，我们可根据任务需求来采用不同的截断点,例如若我们更重视“查准率”，则可选择排序中靠前的位置进行截断;若更重视“查全率"，则可选择靠后的位置进行截断.因此，排序本身的质量好坏，体现了综合考虑学习器在不同任务下的“期望泛化性能”的好坏，或者说，“一般情况下”泛化性能的好坏.ROC曲线则是从这个角度出发来研究学习器泛化性能的有力工具.ROC曲线的纵轴是“真正例率”(TruePositiveRate,简称TPR),横轴是“假正例率”(FalsePositiveRate,简称FPR)文章知识点与官方知识档案匹配，可进一步学习相关知识OpenCV技能树首页概览29612人正在系统学习中
关键词: 机器学习, 监督学习, 无监督学习, 特征工程
AI技术: 机器学习, 有监督学习, 无监督学习, 半监督学习, 强化学习
行业: 机器学习, 监督学习, 无监督学习
重大事件摘要: 这篇文章主要介绍了机器学习的基础知识，包括机器学习的定义、分类、一般流程、特征工程、模型性能判别与选择以及模型性能评价等方面的内容。以下是文章中的重大事件总结：

1. **机器学习定义**：
   - 机器学习是一种让计算机自动从数据中总结规律并得出预测模型的方法，是实现人工智能的一种方式。
   - 它是一门交叉学科，综合了统计学、概率论、逼近论、凸分析、计算复杂性理论等多个领域。

2. **机器学习分类**：
   - **有监督学习 (Supervised Learning)**：使用有标签的数据训练模型，用于回归问题和分类问题。常见算法包括线性回归、逻辑回归、K-近邻、朴素贝叶斯、决策树、随机森林、支持向量机等。
   - **无监督学习 (Unsupervised Learning)**：没有给定类标训练样本，直接对数据建模。常见算法包括K-means、EM算法等。
   - **半监督学习 (Semi-supervised Learning)**：介于有监督学习和无监督学习之间，数据集包含有标签和无标签的数据。
   - **强化学习 (Reinforcement Learning)**：通过不断学习和总结规律，最终学会的过程。强调基于环境做出相应的动作。

3. **机器学习一般流程**：
   - 特征工程是关键步骤，任务是从原始数据中提取最具代表性的特征。
   - 数据预处理包括归一化、标准化、离散化和二值化等方法。
   - 特征工程包括特征构造、特征提取和特征选择三个子问题。

4. **模型性能判别与选择**：
   - 错误率、精度、误差（训练误差和泛化误差）等基础概念。
   - 过拟合与欠拟合的问题及解决方法。
   - 常用的模型选择方法包括留出法、正则化、交叉验证和自助法。

5. **模型性能评价**：
   - 错误率与精度。
   - 查准率、查全率、F1值和P-R曲线。
   - ROC曲线及其在评估学习器泛化性能中的应用。

6. **进一步学习资源**：
   - 文章最后提到了OpenCV技能树首页概览，供读者系统学习相关知识。

这篇文章为读者提供了一个全面的机器学习基础知识框架，适合初学者了解和入门机器学习领域。
