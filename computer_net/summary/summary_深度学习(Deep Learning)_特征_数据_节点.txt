标题: 深度学习(Deep Learning)_特征_数据_节点
链接: http://www.baidu.com/link?url=ypi4uhjoaSTcqDQ1o2Tom2dYe38V-GM6YzVAtqPyob3ioqI9SA28OHqzuG9sx3m1lYYJgJBHnJQU3QwbsL2GVK
总结: 深度学习（DL，DeepLearning）特指基于深层神经网络模型和方法的机器学习。它是在统计机器学习、人工神经网络等算法模型基础上，结合当代大数据和大算力的发展而发展出来的。深度学习最重要的技术特征是具有自动提取特征的能力。2010年以来，深度学习被认为是解决强人工智能这一重大科技问题的最具潜力的技术途径，也是计算机、大数据科学和人工智能等学科领域的研究热点。神经网络算法、算力和数据是开展深度学习的三要素。深度学习学习样本数据的内在规律和表示层次，这些学习过程中获得的信息对诸如文字、图像和声音等数据的解释有很大的帮助。它的最终目标是让机器能够像人一样具有分析学习能力，能够识别文字、图像和声音等数据。深度学习是一套复杂的机器学习算法，在语音和图像识别方面取得的效果，远远超过先前相关技术。深度学习在计算机视觉、自然语言处理、多模态数据分析、科学探索等领域，以及其他相关领域都取得了很多成果。深度学习使机器模仿视听和思考等人类的活动，解决了很多复杂的模式识别难题，使得人工智能相关技术取得了很大进步。深度学习的不可解释性是其饱受诟病的重要因素。深度学习是一类模式分析方法的统称，就具体研究内容而言，主要涉及三类方法：（1）基于卷积运算的神经网络系统，即卷积神经网络（CNN）。（2）基于多层神经元的自编码神经网络，包括自编码（Autoencoder）以及近年来受到广泛关注的稀疏编码两类（SparseCoding）。（3）以多层自编码神经网络的方式进行预训练，进而结合鉴别信息进一步优化神经网络权值的深度置信网络（DBN）。通过多层处理，逐渐将初始的“低层”特征表示转化为“高层”特征表示后，用“简单模型”即可完成复杂的分类等学习任务。由此可将深度学习理解为进行“特征学习”（featurelearning）或“表示学习”（representationlearning）。以往在机器学习用于现实任务时，描述样本的特征通常需由人类专家来设计，这成为“特征工程”（featureengineering）。众所周知，特征的好坏对泛化性能有至关重要的影响，人类专家设计出好特征也并非易事；特征学习（表征学习）则通过机器学习技术自身来产生好特征，这使机器学习向“全自动数据分析”又前进了一步。近年来，研究人员也逐渐将这几类方法结合起来，如对原本是以有监督学习为基础的卷积神经网络结合自编码神经网络进行无监督的预训练，进而利用鉴别信息微调网络参数形成的卷积深度置信网络。与传统的学习方法相比，深度学习方法预设了更多的模型参数，因此模型训练难度更大，根据统计学习的一般规律知道，模型参数越多，需要参与训练的数据量也越大。20世纪八九十年代由于计算机计算能力有限和相关技术的限制，可用于分析的数据量太小，深度学习在模式分析中并没有表现出优异的识别性能。自从2006年，Hinton等提出快速计算受限玻耳兹曼机（RBM）网络权值及偏差的CD-K算法以后，RBM就成了增加神经网络深度的有力工具，导致后面使用广泛的DBN（由Hinton等开发并已被微软等公司用于语音识别中）等深度网络的出现。与此同时，稀疏编码等由于能自动从数据中提取特征也被应用于深度学习中。基于局部数据区域的卷积神经网络方法近年来也被大量研究。深度学习是机器学习的一种，而机器学习是实现人工智能的必经路径。深度学习的概念源于人工神经网络的研究，含多个隐藏层的多层感知器就是一种深度学习结构。深度学习通过组合低层特征形成更加抽象的高层表示属性类别或特征，以发现数据的分布式特征表示。研究深度学习的动机在于建立模拟人脑进行分析学习的神经网络，它模仿人脑的机制来解释数据，例如图像，声音和文本等。含多个隐层的深度学习模型从一个输入中产生一个输出所涉及的计算可以通过一个流向图（flowgraph）来表示：流向图是一种能够表示计算的图，在这种图中每一个节点表示一个基本的计算以及一个计算的值，计算的结果被应用到这个节点的子节点的值。考虑这样一个计算集合，它可以被允许在每一个节点和可能的图结构中，并定义了一个函数族。输入节点没有父节点，输出节点没有子节点。这种流向图的一个特别属性是深度（depth）：从一个输入到一个输出的最长路径的长度。传统的前馈神经网络能够被看作拥有等于层数的深度（比如对于输出层为隐层数加1）。SVMs有深度2（一个对应于核输出或者特征空间，另一个对应于所产生输出的线性混合）。人工智能研究的方向之一，是以所谓“专家系统”为代表的，用大量“如果-就”（If-Then）规则定义的，自上而下的思路。人工神经网络（ArtificialNeuralNetwork），标志着另外一种自下而上的思路。神经网络没有一个严格的正式定义。它的基本特点，是试图模仿大脑的神经元之间传递，处理信息的模式。人工智能是一个学科领域，广义上讲一切能够模拟人、替代人的技术、方法和系统都属于人工智能的范畴。人工智能的概念从1956年在达特莫斯会议上被提出后历经了漫长的发展过程，出现了多种多样的技术方法，也获得了广泛的应用。但现阶段科技界和工业界关注的人工智能是一种强人工智能或者是通用人工智能。深度学习是人工智能研究中一条技术途径，也是2012年以后被认为是解决强人工智能的最有潜力的途径。深度学习是机器学习的一个重要分支。与其它机器学习方法相比，深度学习最核心的技术特征是具有自动提取特征的能力。由于特征表示与提取是许多机器学习方法的基础，也是重要难题。长期以来。结合不同行业和应用领域知识，由专业技术人员人工设计和提取特征是前深度学习时代的一项重要研究任务，这一工作也被称为特征工程。深度学习的强大之处在于将特征工程这一费时费力的工作转变为由机器自动完成，这也是深度学习得以广泛应用的核心价值。神经网络的发展远早于深度学习。但在神经网络技术发展的初期，仅限于浅层的神经网络模型（通常不超过3层）。最典型的浅层神经网络是感知机模型。感知机模型也是最简单的神经网络模型，它不仅只有1层，而且只有1个神经元，该模型最早在1958年提出；另一种典型的浅层神经网络是多层感知机模型，也即拥有2个计算层的神经网络模型。神经网络模型的层数逐渐变多，并通过反向传播算法用于机器学习任务便诞生了令世人瞩目的深度学习技术。区别于传统的浅层学习，深度学习的不同在于：（1）强调了模型结构的深度，通常有5层、6层，甚至10多层的隐层节点；（2）明确了特征学习的重要性。也就是说，通过逐层特征变换，将样本在原空间的特征表示变换到一个新特征空间，从而使分类或预测更容易。与人工规则构造特征的方法相比，利用大数据来学习特征，更能够刻画数据丰富的内在信息。通过设计建立适量的神经元计算节点和多层运算层次结构，选择合适的输入层和输出层，通过网络的学习和调优，建立起从输入到输出的函数关系，虽然不能100%找到输入与输出的函数关系，但是可以尽可能的逼近现实的关联关系。使用训练成功的网络模型，就可以实现我们对复杂事务处理的自动化要求。典型的深度学习模型有卷积神经网络（convolutionalneuralnetwork）、DBN和堆栈自编码网络（stackedauto-encodernetwork）模型等，下面对这些模型进行描述。卷积神经网络模型卷积神经网络模型在无监督预训练出现之前，训练深度神经网络通常非常困难，而其中一个特例是卷积神经网络。卷积神经网络受视觉系统的结构启发而产生。第一个卷积神经网络计算模型是在Fukushima的神经认知机中提出的，基于神经元之间的局部连接和分层组织图像转换，将有相同参数的神经元应用于前一层神经网络的不同位置，得到一种平移不变神经网络结构形式。后来，LeCun等人在该思想的基础上，用误差梯度设计并训练卷积神经网络，在一些模式识别任务上得到优越的性能。至今，基于卷积神经网络的模式识别系统是最好的实现系统之一，尤其在手写体字符识别任务上表现出非凡的性能。深度信任网络模型DBN可以解释为贝叶斯概率生成模型，由多层随机隐变量组成，上面的两层具有无向对称连接，下面的层得到来自上一层的自顶向下的有向连接，最底层单元的状态为可见输入数据向量。DBN由若2F结构单元堆栈组成，结构单元通常为RBM（RestrictedBoltzmannMachine，受限玻尔兹曼机）[12]。堆栈中每个RBM单元的可视层神经元数量等于前一RBM单元的隐层神经元数量。根据深度学习机制，采用输入样例训练第一层RBM单元，并利用其输出训练第二层RBM模型，将RBM模型进行堆栈通过增加层来改善模型性能。在无监督预训练过程中，DBN编码输入到顶层RBM后，解码顶层的状态到最底层的单元，实现输入的重构。RBM作为DBN的结构单元，与每一层DBN共享参数。堆栈自编码网络模型堆栈自编码网络的结构与DBN类似，由若干结构单元堆栈组成，不同之处在于其结构单元为自编码模型（auto-en-coder）而不是RBM。自编码模型是一个两层的神经网络，第一层称为编码层，第二层称为解码层。2006年，Hinton提出了在非监督数据上建立多层神经网络的一个有效方法，具体分为两步：首先逐层构建单层神经元，这样每次都是训练一个单层网络；当所有层训练完后，使用wake-sleep算法进行调优。将除最顶层的其他层间的权重变为双向的，这样最顶层仍然是一个单层神经网络，而其他层则变为了图模型。向上的权重用于“认知”，向下的权重用于“生成”。然后使用wake-sleep算法调整所有的权重。让认知和生成达成一致，也就是保证生成的最顶层表示能够尽可能正确的复原底层的节点。比如顶层的一个节点表示人脸，那么所有人脸的图像应该激活这个节点，并且这个结果向下生成的图像应该能够表现为一个大概的人脸图像。wake-sleep算法分为醒（wake）和睡（sleep）两个部分。wake阶段：认知过程，通过外界的特征和向上的权重产生每一层的抽象表示，并且使用梯度下降修改层间的下行权重。sleep阶段：生成过程，通过顶层表示和向下权重，生成底层的状态，同时修改层间向上的权重。自下上升的非监督学习就是从底层开始，一层一层地往顶层训练。采用无标定数据（有标定数据也可）分层训练各层参数，这一步可以看作是一个无监督训练过程，这也是和传统神经网络区别最大的部分，可以看作是特征学习过程。具体的，先用无标定数据训练第一层，训练时先学习第一层的参数，这层可以看作是得到一个使得输出和输入差别最小的三层神经网络的隐层，由于模型容量的限制以及稀疏性约束，使得得到的模型能够学习到数据本身的结构，从而得到比输入更具有表示能力的特征；在学习得到n-l层后，将n-l层的输出作为第n层的输入，训练第n层，由此分别得到各层的参数。自顶向下的监督学习就是通过带标签的数据去训练，误差自顶向下传输，对网络进行微调。基于第一步得到的各层参数进一步优调整个多层模型的参数，这一步是一个有监督训练过程。第一步类似神经网络的随机初始化初值过程，由于第一步不是随机初始化，而是通过学习输入数据的结构得到的，因而这个初值更接近全局最优，从而能够取得更好的效果。所以深度学习的良好效果在很大程度上归功于第一步的特征学习的过程。计算机视觉香港中文大学的多媒体实验室是最早应用深度学习进行计算机视觉研究的华人团队。在世界级人工智能竞赛LFW（大规模人脸识别竞赛）上，该实验室曾力压FaceBook夺得冠军，使得人工智能在该领域的识别能力首次超越真人。语音识别微软研究人员通过与hinton合作，首先将RBM和DBN引入到语音识别声学模型训练中，并且在大词汇量语音识别系统中获得巨大成功，使得语音识别的错误率相对减低30%。但是，DNN还没有有效的并行快速算法，很多研究机构都是在利用大规模数据语料通过GPU平台提高DNN声学模型的训练效率。在国际上，IBM、google等公司都快速进行了DNN语音识别的研究，并且速度飞快。国内方面，阿里巴巴、科大讯飞、百度、中科院自动化所等公司或研究单位，也在进行深度学习在语音识别上的研究。自然语言处理等其他领域很多机构在开展研究，2013年，TomasMikolov、KaiChen、GregCorrado、JeffreyDean发表论文EfficientEstimationofWordRepresentationsinVectorSpace建立word2vector模型，与传统的词袋模型（bagofwords）相比，word2vector能够更好地表达语法信息。深度学习在自然语言处理等领域主要应用于机器翻译以及语义挖掘等方面。2020年，深度学习可以加速半导体封测创新。在降低重复性人工、提高良率、管控精度和效率、降低检测成本方面，AI深度学习驱动的AOI具有广阔的市场前景，但驾驭起来并不简单。2020年4月13日，英国《自然·机器智能》杂志发表的一项医学与人工智能（AI）研究中，瑞士科学家介绍了一种人工智能系统可以几秒之内扫描心血管血流。这个深度学习模型有望让临床医师在患者接受核磁共振扫描的同时，实时观察血流变化，从而优化诊断工作流。返回搜狐，查看更多责任编辑：平台声明：该文观点仅代表作者本人，搜狐号系信息发布平台，搜狐仅提供信息存储空间服务。阅读(90)内容举报
关键词: 深度学习，神经网络，特征学习，自动提取特征
AI技术: 深度学习, 卷积神经网络, 受限玻尔兹曼机深度信任网络堆栈自编码网络
行业: 计算机视觉, 语音识别, 自然语言处理
重大事件摘要: 这篇文章主要介绍了深度学习（Deep Learning）的概念、发展、技术特征、应用领域以及与人工智能的关系。以下是文章中提到的几大重要事件：

1. **深度学习的提出与发展**：
   - 深度学习是基于深层神经网络模型和算法的机器学习技术，结合了统计机器学习、人工神经网络和大数据的发展。
   - 2010年以来，深度学习被认为是解决强人工智能问题的技术途径，广泛应用于计算机视觉、语音识别、自然语言处理等领域。

2. **深度学习的技术特征**：
   - 深度学习具有自动提取特征的能力，这是其最核心的技术特征。
   - 它通过多层处理将初始“低层”特征表示转化为“高层”特征表示，从而完成复杂的分类等任务。

3. **深度学习的主要方法**：
   - 基于卷积运算的神经网络系统（卷积神经网络，CNN）。
   - 基于多层神经元的自编码神经网络（自编码器，Autoencoder）。
   - 深度置信网络（DBN），通过多层训练进行预训练，再结合鉴别信息进一步优化网络权值。

4. **深度学习的应用**：
   - 在计算机视觉领域表现卓越，如手写体字符识别。
   - 在语音识别领域显著降低了错误率。
   - 在自然语言处理领域，如机器翻译和语义挖掘等方面有广泛应用。
   - 在医学领域，如心血管血流检测中展现了巨大潜力。

5. **深度学习与传统机器学习的区别**：
   - 传统机器学习依赖于专家设计的特征，而深度学习通过学习技术自动生成好的特征。
   - 深度学习预设了更多的模型参数，需要更多的数据量参与训练。

6. **深度学习的不可解释性**：
   - 尽管深度学习在许多领域取得了显著成果，但其内部工作机制复杂且难以解释，这是其饱受诟病的重要因素之一。

7. **深度学习的硬件需求**：
   - 由于模型参数多，训练难度大，深度学习对计算资源的需求很高，通常需要利用GPU等硬件加速计算。

8. **深度学习的历史背景**：
   - 深度学习的发展可以追溯到人工神经网络的研究，特别是多层感知机（MLP）。
   - 20世纪八九十年代由于计算能力限制，深度学习未能充分发挥作用，直到2006年Hinton等人提出的快速计算受限玻耳兹曼机（RBM）的CD-K算法，使得深度网络成为可能。

9. **卷积神经网络（CNN）**：
   - CNN受视觉系统启发，通过局部连接和分层组织图像转换，实现平移不变性。
   - LeCun等人基于误差梯度设计并训练了卷积神经网络，在模式识别任务上表现出色。

10. **深度信任网络（DBN）**：
    - DBN是一种贝叶斯概率生成模型，由多层随机隐变量组成，采用输入样例进行无监督预训练，再通过有监督数据进行微调。

11. **堆栈自编码网络（Stacked Auto-Encoder Network）**：
    - 结构类似DBN，但使用自编码模型作为单元，通过无监督预训练和有监督微调进行训练。

12. **计算机视觉领域的突破**：
    - 香港中文大学的多媒体实验室在LFW竞赛中夺冠，展示了深度学习在计算机视觉中的优越性能。

13. **语音识别的进步**：
    - 微软与Hinton合作，将RBM和DBN引入语音识别，显著降低了错误率，各大科技公司纷纷跟进研究。

14. **自然语言处理的应用**：
    - Tomas Mikolov等人提出了word2vector模型，提升了自然语言处理的效果。

15. **医学领域的应用前景**：
    - 英国《自然·机器智能》杂志发表的研究展示了深度学习在心血管血流检测中的应用潜力。

这些事件共同构成了深度学习从概念提出到技术成熟，再到广泛应用的发展脉络。
