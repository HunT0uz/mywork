标题: 深度学习:bert模型-CSDN博客
链接: http://www.baidu.com/link?url=dfoviCuhFtSusHAKeuvqS-Is14i3ar0IGZAgQ8kNx01L_VZmbqERFddf30tmC9gtl5rdpLADR73vrYgHd-p09rS7koNVRXxQkV3lG78pTNC
总结: multi-headed机制1、通过不同的head得到多个特征表达，一般8个head2、将所有特征拼接在一起3、降维，将Z0~Z7连接一个FC全连接实现降维多层堆叠位置编码如何实现位置编码？（1）为每个时间步添加一个0-1范围内的数字，其中0表示第一个单词，1表示最后一个单词。我喜欢吃洋葱【00.160.32.....1】我真的不喜欢吃洋葱【00.1250.25.....1】问题：我们可以看到，如果句子长度不同，那么位置编码是不一样，所以无法表示句子之间有什么相似性。（2）1-n正整数范围分配我喜欢吃洋葱【1，2，3，4，5，6】&nbsp;&nbsp;&nbsp;我真的不喜欢吃洋葱【1，2，3，4，5，6，7】&nbsp;&nbsp;&nbsp;问题：往往句子越长，后面的值越大，数字越大说明这个位置占的权重也越大，这样的方式无法凸显每个位置的真实的权重。三角函数位置编码wordembedding：是词向量，由每个词根据查表得到posembedding：就是位置编码。composition：wordembedding和posembedding逐点相加得到，既包含语义信息又包含位置编码信息的最终矩阵pos：指当前字符在句子中的位置（如：”你好啊”，这句话里面“你”的pos=0），dmodel：指的是wordembedding的长度（例“民主”的wordembedding为[1,2,3,4,5]，则dmodel=5），2i表示偶数，2i+1表示奇数。取值范围：i=0,1,...,dmodel−1。偶数使用公式（1），奇数时使用公式（2）。当pos=3,dmodel=128时PositionalEncoding(或者说是posembedding)的计算结果为：优点：1、可以使PE分布在[0,1][0,1]区间。2、不同语句相同位置的字符PE值一样(如：当pos=0时，PE=0)。训练数据训练数据集是什么？（方法1）随机的将句子中的15%的词汇进行mask。让模型去预测mask的词汇。注：一般选择字进行mask，词的可能性太多，例如今天，明天，后天，上午，下午，没有，再次等等。方法2）预测两个句子是否应该连在一起。CLS：分类标记（ClassificationToken）用于表示输入序列的开始。在输入序列中，CLS应放置在句子的开头。在训练过程中，CLS也当作一个词参与训练，得到对应与其他词汇关系的词向量。SEP：分隔符标记（SeparatorToken）用于分隔两个句子或表示单个句子的结束。在处理多个句子时SEP应放置在每个句子的结尾。在训练过程中，SEP也当作一个词参与训练，得到对应与其他词汇关系的词向量。文章知识点与官方知识档案匹配，可进一步学习相关知识算法技能树首页概览65067人正在系统学习中
关键词: BERT模型, 位置编码, 训练数据, 词向量
AI技术: 深度学习，BERT模型，位置编码，词向量，CLS标记
行业: 深度学习, BERT模型, CSDN博客
重大事件摘要: 这篇文章主要介绍了深度学习中的BERT模型，特别是其multi-headed机制和位置编码的实现方式。文章首先解释了multi-headed机制的工作原理，即通过不同的头得到多个特征表达，然后将所有特征拼接在一起，并通过一个全连接层进行降维。接着，文章详细介绍了位置编码的实现方式，包括为每个时间步添加一个0-1范围内的数字、使用1-n正整数范围分配以及三角函数位置编码。此外，文章还介绍了训练数据的选择和处理方法，包括随机将句子中的15%的词汇进行mask和预测两个句子是否应该连在一起。最后，文章提到了CLS和SEP标记在输入序列中的作用。
